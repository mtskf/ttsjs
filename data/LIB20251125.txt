週刊Life is beautiful ２０２５年１１月２５日号：The Dawn of the AI-Native Operating System


今週のざっくばらん

The Dawn of the AI-Native Operating System

ChatGPTがOpenAIから発表されて3年が経とうとしています。その後、Function Call、MCP、Code Interpreter、Artifact、Multi-modal LLM、Agentなど、数多くの仕組みが生まれました。しかし、ChatGPTの基本的なインターフェイスはいまだに「テキストベースのチャット」に留まっています。

私たちソフトウェア・エンジニアの間で、Claude Codeのようなチャット形式のプログラミング環境が広く使われている事実は、LLMと人間の間の**自然言語インターフェイス（Natural Language Interface）**が、既存のGUIを超えるポテンシャルを持つことを示しています。

一方、MicrosoftのCoPilotのように、従来のアプリケーション（Excelなど）に自然言語インターフェイスを後付けする試みは、必ずしも成功していません。それは、これらのアプリケーションが「LLMが存在しない時代の設計思想」に基づいているからです。

同じ理由で、LLMにブラウザや既存アプリを操作させるアプローチにも限界があります。真にAIネイティブなアプローチとは、アプリケーションの機能をAPIとしてLLMに提示し、ユーザーの意図（Intent）を一連のAPIコールに変換することです。LLMが、ユーザーとアプリケーションの間のインタープリタの役割を果たすのです。

つまり、自然言語を理解するLLMの存在を前提としたAI-NativeなコンピュータのUser Experienceは、ユーザーとLLMの間のNLUIと、ユーザーとアプリケーションの間のGUIを融合したユーザー体験を提供すべきであり、アプリケーションのアーキテクチャそのものも従来とは大きく異なるものである必要があるのです。

本稿では、「AI-Nativeなコンピュータ（OS）のUser Experienceとアプリケーション・アーキテクチャ」という視点から、業界全体が向かうべき方向性を示すために設計された**MulmoChat（Multi-modal Chat）**というオープンソースのプロトタイプを紹介します。

NLUIとGUIの融合

Function CallとMCP（以後はTool、もしくはTool Callと呼びます）は、LLMに外部機能を提供する仕組みとして登場しました。これらは、JSON形式のパラメータを受け取り、JSON形式の結果を返すという、テキストベースのインターフェイスです。

Tool Callの結果として、アプリケーション独自のウィンドウを表示したり、ブラウザーで特定のページを開くことは可能ですが、それらはチャットインターフェイスとは切り離されており、「NLUI（Natural Language UI）とGUIの融合」とは呼べません。

この問題を解決するために、MulmoChatではTool Callの仕組みを拡張し、NLUIとGUIの自然な統合を実現しました。具体的には、Tool Callのreturn valueを拡張し、LLMへの返答に加えて、ユーザーに表示すべきTool-specificなデータを返すことを可能にしています。このデータの表示は、システムに登録されたTool専用のビューアーが担当します。

このアーキテクチャにより、アプリケーションはチャット（NLUI）の流れの中で、独自のインタラクティブなGUIをリアルタイムに生成・操作できるようになります。

以下は、Excelに相当する表計算アプリケーションを、このアーキテクチャ上で動作させた場合の一例です。

「SpreadSheet」アプリケーションは、LLMが呼び出せるtoolと、tool-specificなデータを表示するビューアーを登録する。
ユーザーが「Shows me the present value of $1000 monthly income over a year, making it easy to change the discount rate.」と指示を与える。
LLMはユーザーの意図（intent）を解釈し、"SpreadSheet" toolを呼び出すためのJSONデータを構築する。
Toolは、LLMに対して「表計算データを生成・表示中である」と通知し、システムに対してはLLMが生成したスプレッドシートデータを返す。
システムは返されたデータタイプを判定し、対応する専用ビューアーを起動する。
ビューアーが生成されたスプレッドシートをユーザーに表示する。
ユーザーはGUI上で直接操作を行い、その結果が再びチャットコンテキスト（NLUI）にフィードバックされる。
このように、MulmoChatでは、LLM・GUI・ユーザーの三者が同一の文脈で連続的にやりとりできます。これこそが、NLUIとGUIの真の融合であり、AI-Nativeなコンピューティング環境における新しいユーザー体験の核となります。



Beyond the Sea of App Icons

上のシナリオは、「ユーザー体験」という観点から、既存のシステムと大きく異なります。

多くのiPhoneユーザーは、数十から百個を超えるアプリケーションをインストールしています。そのため、何かをするたびに「無数のアプリ・アイコンの海の中から、必要なものを探し出す」という煩雑な作業を繰り返さなければなりません。 AI-Nativeなシステムでは、どのToolを使うかの判断はLLMが行います。

ユーザーは「アプリを選んで起動する」という作業から完全に解放されるのです。

表計算やプレゼン資料、文書作成などは、LLMが利用できるToolとしてシステムに登録されているだけで、ユーザーは「アプリケーション」という概念そのものから自由になります。

さらに、スプレッドシートやプレゼン資料のドラフトは、LLMが自動的に生成してくれるため、Learning Curve（学習コスト）は桁違いに低くなります。

下の図は、MulmoChatに対してユーザーが“Make a travel guide for Tokyo with pictures of 3 famous landmarks.”と指示した際の出力例です。Wordに相当するToolが呼び出され、わずか数十秒でドキュメントが生成されています。



ユーザーはこのドキュメントを起点に、情報を追記したり、LLMにさらなる指示を出して内容を更新できます。これにより、チャットがそのままドキュメント生成プロセスになるのです。

この仕組みは、ClaudeのArtifactやChatGPTのCanvasに似ていますが、二つの点で大きく異なります。

Extensibility（拡張性）
ArtifactやCanvasは限定されたデータタイプにしか対応していませんが、MulmoChatの仕組みはサードパーティが新しいデータタイプやビューアーを自由に追加できます。

Communication-first（対話中心）
ArtifactやCanvasは成果物の生成を目的としていますが、MulmoChatはLLMとユーザーのコミュニケーションを中心に設計されています。生成されるドキュメントやソフトウェアは、あくまでその対話の一部にすぎません。

この設計により、たとえば旅行代理店が顧客向け資料を作るだけでなく、LLMが“旅行代理店そのもの”としてユーザーに提案を行うことが可能になります。

さらに、この仕組みはLLMがユーザーから情報を得る場面にも使えます。下の例は、MulmoCast内で「病院のレセプション」として動作するロールですが（MulmoCastは、同時にLLMに提示するToolsを制限したり特定のシステムプロンプトを与えることにより、特定の「役割」を行わせています）、LLMはユーザーから情報を得るためのフォームをダイナミックに生成し、自然な対話の流れで提示します。



このように、MulmoChatでは情報の生成と収集が、同じ自然言語インターフェイス上でシームレスに行われる。これこそが、AI-Nativeコンピューティング環境における「NLUIとGUIが融合されたユーザー体験」です。

Domain-Specific Presentation Language ― The Bridge Between Intent and Interface

このアーキテクチャにおいて極めて重要な役割を果たすのが、**LLMがユーザーに情報（入力フォームを含む）を提示する際に生成するDSL（Domain-Specific Language）**です。より正確には、表示に特化した言語であることから、**Domain-Specific Presentation Language（DSPL）**と呼ぶのが適切かもしれません。

スプレッドシート、ドキュメント、フォームなど、それぞれのToolが必要とするデータ形式は異なります。各Toolはそのスキーマを定義としてLLMに提示しており、LLMはユーザーのリクエストに応じてそのスキーマに準拠したデータを生成し、Toolを呼び出します。

多くの場合、Toolはその生成データをそのままシステムにTool-specificなデータとして渡し、専用のビューアーがそのデータを画面に描画し、ユーザーとのやり取りを担います。

この過程では、以下の二つの変換が行われています。

LLM: ユーザーの意図（intent）をDSLへ変換する。
Viewer: DSLをGUIへ変換する。
つまり、DSLと専用ビューアーの組み合わせこそが、テキスト（JSON）生成を得意とするLLMにインタラクティブなGUI表現を可能にしているのです。

DSLとしては、ドキュメント作成に使うMarkdownをはじめ、HTML、SVG、TeXなどの汎用的でオープンな形式を採用することも、特定のTool専用の独自DSLを用いることも可能です。

汎用的な形式はLLMがすでに学習しているため出力精度が高い一方で、構文が冗長でトークン数が増えやすく、期待通りの表現を得にくい場合もあります。そのため、MulmoChatでは目的に応じて汎用DSLと専用DSLを柔軟に使い分ける設計にしています。

LLM開発の潮流は、Tool Callとコード生成能力の強化に向かっていますが、DSL活用はこの流れと極めて相性が良いのです。実際、MulmoChatの開発を通じて、GPT-5やSonnet 4.5といった最先端モデルだけでなく、gpt-oss:20bやqwen3:30bのような中小規模モデルでも十分な性能が発揮されることが確認されています。

The Road Ahead ― Toward an AI-Native Computing Paradigm

MulmoChatで試みていることは、単なるチャットインターフェイスの拡張ではありません。それは、「OS」という概念そのものを再定義する挑戦です。

これまでのコンピュータは、「人間がアプリケーションを操作する」ことを前提としていました。しかし、AI-Nativeな世界では、LLMが人間の意図を理解し、適切なToolを組み合わせて結果を提示する。ユーザーはもはやアプリを探す必要も、操作手順を覚える必要もありません。

この変化は、MS-DOSからGUIへの転換に匹敵します。かつて「文字入力」が「マウス操作」に置き換わったように、今度は「自然言語」と「DSL」によって、思考そのものがOSの入力になるのです。

MulmoChatはその未来を具体化するための「OSレベルの実験場」であり、ここで提案されるDSPLやToolアーキテクチャは、今後のAI-Nativeコンピューティングの基盤になるでしょう。

最終的に目指すのは、**「人間とAIが、同じキャンバス上で思考を共有する世界」**です。NLUIとGUIが融合した環境では、創造、教育、研究、そして日常のすべてが「対話」という自然な流れに溶け込んでいきます。

【追記】上の文章を書いた直ぐ後に、Googleから「Generative UI: LLMs are Effective UI Generators」という論文が発表されました。「LLMと人間の対話をもっとリッチなものにしたい」という観点からはMulmoScript/MulmoChatと目指しているところは同じですが、HTMLという汎用のプレゼンテーション言語を使っている点が、私のアプローチと異なります。もちろん、MulmoChatもHTMLをサポートしていますが、HTMLだけでは限界があると私は考えており、MulmoScriptや拡張したMarkdown(ImagePromptの埋め込みをサポートするMarkdown)の方が、良いケースが多々あるし、プラグインの仕組みで拡張性を持たせるべきだと考えています。

私の目に留まった記事

Apple Nears $1 Billion-a Year Deal to Use Google AI for Siri

AppleがSiriのバックエンドとして、GoogleのGemini（1.2兆パラメータのカスタムモデル）を使うことになり、年間$1billionの使用料を支払うことになる、というリーク情報（＝公式な発表ではない）を紹介する記事です。

Appleが以前から、Google、OpenAI、AnthropicのAIの評価をしていたことは知られていますが、AppleとしてはLLMをコモディティ扱いし、最も安価にサービスを提供する会社を選ぶだろうと言われていました。

GeminiはAppleのPrivate Cloud Compute上で動作し、ユーザーデータはGoogle側に渡らない設計であり、AppleがGoogleを単なる「技術の提供者」として扱っていることが分かります。そのサーバーが、GoogleのTPUを使っているのか、Apple独自のシリコンを使っているのか、それともNVIDIAのGPUなのかについては、この記事には書かれていません。

ちなみに、Apple自身も独自のLLMを並行して開発しており、準備ができ次第、そちらに切り替える予定だともこの記事には書かれています。

Microsoft, NVIDIA and Anthropic announce strategic partnerships

毎週のように、大規模な投資を伴う発表がされていますが、この、Microsoft、NVIDIA、Anthropicの三社によるパートナーシップもその一つです。具体的には、

NVIDIAがAnthropicに、$10billion投資
MicrosoftがAnthropicに、$5billion投資
Anthropicが$30billion分の計算能力をMicrosoftから購入
Microsoftはその計算能力をNVIDIAのGPUを使って構築
Microsoftは、Copilot上でAnthropicのClaudeを使えるようにする
というものです。

この業界で最近よく見る「ベンダー・ファイナンス（商品の売り手が顧客に購入資金を融資や投資の形で提供すること）」です。「循環取引」だと批判する人がいますが、それほど神経質になる必要はないと思います。

Microsoftは、AnthropicのライバルであるOpenAIの大株主ですが、企業向けのAIとしては、AnthropicのClaudeの方が評価が高いことを受け、顧客に対して、Claudeも選べるようにすることは当然です。

Anthropicは、これまで大株主であるAmazonのAIチップ（TrainiumとInferentia）を使っていたことが知られていますが、最近になって（同じく大株主の）GoogleのTPUも使うことをアナウンスしていました。NVIDIAとしては、このままGoogleに奪われてしまうことは好ましくないため、Microsoftと組んで、AnthropicにNVIDIAのGPUにもコミットメントしてもらうことにしたように私には見えます。

一連の流れを見ていると、各社がNVIDIAの１強状態からの脱却を図ろうとしている中で、NVIDIAが豊富なキャッシュフローを使って、ベンダーファイナンスの形でそれを防止しようとしている中、２番手のAMDだけでなく、GoogleのTPUが着実に駒を進める中、AmazonのカスタムAIチップの影響力が弱っているように見えます。

Jeff Bezos’ Project Prometheus: The AI Startup Targeting the Physical Economy

AmazonのCEOを退いてからは、主に（宇宙事業などの）投資家として活動してきたジェフ・ベゾス氏が、久しぶりに共同CEOとして、Project Prometheusという新しいAIベンチャーに参加することを報道するLinkedIn上の記事です。

既に$6.2billionの資金を調達しており、会社のミッションは、「Physical Economy（物理経済）向けのAI」─つまり、工業・製造・モノづくりの領域でAIを活用することが掲げられているそうです。対象とする応用分野として「コンピュータ」「自動車」「宇宙／航空」など、ハードウェアや製造現場、リアルワールドの物理空間に強く関わるものが挙げられています。

ベゾス氏にとって、これはAmazonのCEOを退任して以来、初めての「実務上の役割（オペレーショナルな役割）」で、同社のもう1人の共同CEOは、Vik Bajaj（物理・化学の博士号を持ち、かつてGoogle XやVerilyに関わっていた人物）です。

採用も進んでおり、既に100人規模の従業員が動いており、うち多くが OpenAI、DeepMind、Meta といったトップAI企業出身者です。

以前からこのメルマガで指摘している通り、AIブームの次のフェーズは、ロボット・ドローン・自動運転車などへのAIの応用であることは明らかで、ジェフ・ベゾス氏としてもそこに大きな可能性を見ているのだと思います。

80% of startups pitching Andreessen Horowitz are running on Chinese open-source models.

AIに関して、「OpenAI vs Anthropic vs Googleという構図はもう古い」と指摘する興味深い記事です。

シリコンバレーのVC、Andreessen Horowitz（a16z）によると、彼らに対して資金調達のためのピッチを行うベンチャー企業の8割が、中国製オープンソースモデル（例：DeepSeek）を利用しているそうです。

理由は大きく異なるコストで、APIを利用した場合、OpenAIのものを使うと100万トークンあたり$30なのに対して、中国製のものを使えば、$0.14と桁違いに節約できるため、毎月1億トークン使うスタートアップでは、$1,400 vs $300,000という圧倒的差になります。

OpenAIやAnthropicが、AGIと呼ばれる超知能の構築を目指して莫大な先行投資をする中、中国の企業は、「効率最適化」を最優先して開発をしており、「GPT-4並の性能を持ちながらコストは2%」であるモデルを提供することに成功しているとこの記事は指摘しています。

米国のAI企業が、「高い知能」を参入障壁と考えてビジネスを構築する中、中国企業の戦略は「低いコスト」を参入障壁と考えて戦っているのです。

コンピュータ業界の常ですが、「桁違いに安いコスト」はこれまで不可能だったアプリケーションを可能にするため、今後は、中国製のAIモデルが、これまでにないアプリケーションを生み出し、業界を引っ張って行く可能性が高いと、この記事は指摘しています。

私自身、AIサービスを活用したMulmoCastやMulmoChatを開発していますが、OpenAIやAnthropicのAPIを使っている限り、そのコストゆえに営利ビジネスとして立ち上げるのは難しい状況にあり（月額課金料金が高くなりすぎます）、思い切って、はるかに安価な中国製のモデルを使うことにより、全く異なるビジネスモデルでサービスとして提供するのもありかな、と考えていたところでした。

特にベンチャー企業の場合、最初は赤字を垂れ流しながら成長させるため、集めた資金で18ヶ月突っ走れるのか、3ヶ月しか持たないかは大きな違いです。

AI could wipe out half of all entry-level white-collar jobs and spike unemployment to 10% to 20% in the next one to five years, predicts Anthropic CEO Dario Amodei.

進化し続けるAIの影響で、ここから5年以内に、大学を卒業したばかりの人ができるような初歩的な事務系の仕事の半分がなくなり、失業率が10～20%増えても不思議はないと指摘する、AnthropicのCEO、Dario Amodeiのセリフを紹介するXの投稿です。

失業率の数字そのものがどう変わるかを予測するのは簡単ではありませんが、「初歩的な事務系の仕事」の半分がなくなってしまうことに関しては、私も同様の見方をしています。

プログラミングに関しても、簡単な仕事（例：指示通りに行うリファクタリング）はAIに任せられる時代になったため、これまでだったら、大学を卒業したばかりの新人に任せていたような仕事がなくなってしまいました。

「一人当たりのエンジニアの生産性の向上」という意味では大きなプラスですが、「新人の育成」「新卒エンジニアの仕事場」という観点からは大きなマイナスであり、これが業界全体にどんな影響を与えるのか、少し不安になることがあります。

同様のことは、一般的な事務系の仕事にも言え、毎日のように同じことを行う「定型業務」はAIによる自動化が容易で、それらの仕事がごっそりとなくなることは、労働市場に破壊的な影響を与えます。

Dario Amodeiは、「社会はそれに備えるべき」と警告していますが、特にITに弱い日本の政治家が、この件をどのくらいの危機感を持って見ているかは大いに疑問です。これまでのように、補助金を与えて企業の延命を図ることによる間接的な雇用の確保は、一時凌ぎでしかなく、社会保障システムの抜本的な見直しも含めた大きな政策の転換が必要な段階に来ていると思います。

Verifiability

一つ上の話と密接に関わる話ですが、Andrej Karpathy氏による「どんなタスクが自動化しやすいか＝どんな仕事がAIに奪われやすいか」について語っている、とても意味深い記事です。

「ソフトウェア2.0 (Software 2.0)」と称される、ニューラルネットを使ったプログラミングの時代には、「勾配降下法」などを活用して、ニューラルネットに学習させるという方法でソフトウェアを開発しますが、その際には、タスクが「検証可能 (verifiable)」かどうかが、ソフトウェアの作りやすさに直接的な影響を与えると、彼は指摘しています。

「そのタスクが検証可能」であるとは、

リセット可能 ─ 新しい試みを再スタートできる。
効率的 ─ 多数の試行ができる。
報酬可能 ─ どの試行が良かったかを自動的に評価・報酬できる。
であることを意味し、そんなタスクにおいては、ニューラルネットで非常に高い性能を発揮することが可能です。一つ上に書いた「定型業務」が良い例です。具体的な仕事としては、計算、コーディング、（答えが一つに定まる）パズルを解くような仕事、などが挙げられています。

別の言い方をすれば、「検証可能なタスク」はニューラルネットを活用した自動化が容易で、その手の仕事はAIによる置き換えが加速し、一方で、「創造的」「戦略的」「実世界知識＋文脈＋常識を要するタスク」など、検証可能性が低い仕事は自動化が難しく、人間が活躍し続けることができるのです。

Samsung unveils first 2nm results… enters the main game in leading-edge foundry

Samsung Electronicsが、2nmプロセスの半導体の量産結果を初めて明らかにしたことを報告する記事です。

今回、Samsungが量産に成功した第1世代2nm GAAプロセスは、第2世代3nmと比べて

性能5%向上
電力効率8%改善
面積5%縮小。
という特徴があります。

しかし、ファウンドリビジネスにおけるTSMCとのシェア格差は依然大きく、2024年第2四半期で、以下の通りです。

TSMC：70.2%
Samsung：7.3%
両社とも2nmからGAA（Gate-All-Around）技術（電流漏れを抑え、高性能・高効率を実現する次世代トランジスタ技術）を採用していますが、Samsungは3nmで既に実装済みで、そこに１世代分の経験値がある点が若干有利で、2nm世代で形勢逆転の可能性もあるとこの記事は指摘しています。

TSMCは、米国工場によるコスト増を反映して、2nmウェハ価格を10～20％値上げ予定ですが、Samsungは、柔軟な価格戦略で対抗すると見られています。

Teslaの車載チップ「AI5」はTSMCとSamsungの二社製造ですが、次世代AI6チップはSamsungが独占受注（契約総額165億ドル＝約23兆ウォン）したと報じられています。

Samsungは2026年発売予定のGalaxy S26シリーズに自社設計・製造のExynos 2600（2nmプロセス採用）を搭載予定で、これに成功すれば、Samsungファウンドリ事業の実力証明となるとこの記事は指摘しています。

最大の課題は「歩留まり」で、TSMCが既に約80％の歩留まりで安定量産に到達しているのに対して、Samsungは50～60％台で、改善中とのことです。

ちなみに、日本のラピダスは、3nmを飛び越して2nmの量産を目指していますが、現時点では試作に成功した段階で、2027年の量産を目指しています。

In two years, the level of detail you can capture in a 3D Gaussian Splat has skyrocketed.

このメルマガでも時々触れている「3D Gaussian Splat」という技術の精度がかなり上昇しています。この技術を使うと、３D空間をキャプチャーし、さらに任意の位置・方向からのビューを高速に再生できます。このXの投稿で紹介されているものを見ると、部屋の壁に表示されている文字まではっきりと読めることが分かると思います。

この技術は、Vision ProのようなVRグラスとの相性が良く、将来、バーチャル・ツアーなどを提供する際に重要な役割を果たします。

DRAM Shortage Worsens… Companies Shift from Monthly Contracts to “Bulk Purchases for Six Months”

AI投資ブームでDRAM不足が深刻化し、DRAM価格交渉が月次・四半期契約から半年以上の長期契約へと移行したことを伝えるXの投稿です。

AIデータセンター向けのGPUは、HBM(High Bandwidth Memory)という特殊なDRAMを大量に使うため、爆発的な需要増により、供給不足が起こっていますが、それが一般のDRAMにまで波及しているそうです。

これまで、DRAMは価格を毎月調整する月次契約が一般的でしたが、需要の急増により、契約期間が半年～年単位に拡大しているそうです。

結果として、

SK hynixは既に2025年分の供給契約を完了し、現在は2027年向けた契約を顧客と交渉中。
Samsungも来年分生産分は、ほぼ全量が契約済みで、DRAM価格を40%以上値上げする案を検討中。
とのことです。

長期契約化により、生産・原価・流通の計画が立てやすくなることで、収益性が向上し、業界では2027年まで好業績が続く「スーパーサイクル」入りとの見方が強いそうです。

This is a very important signal.

一つ上の記事と関連しますが、DRAM・NANDなどメモリ価格の急騰により、PCメーカー各社の利益率悪化が懸念されていると、モルガン・スタンレーによるレポートに書かれていることを紹介するXの投稿です。

記事の中には、DELL、HPQ、HPE、Lenovo、Acerなど各社の業績予想と株価予想まで書かれているので（いずれも下方修正）、興味がある方はご覧ください。

パソコンメーカーの中で私が唯一株を持っているAppleについては、大規模調達力や関税減免などにより、コスト上昇の影響は相対的に小さいと予測しています。

AI is reshaping how McKinsey makes money

コンサルティング会社のマッキンゼーによると、同社の収益の約4分の1が、成功報酬型の料金モデルによって発生しているそうです。

従来、コンサルティング会社は「プロジェクトの範囲と期間を定め、そこに対して時間・人員を基準に料金を請求する」モデルを取ってきましたが、最近は、クライアント側が「この成果を出したい」という目標を示し、それを達成できたかどうかに応じて料金が決まるという形にシフトしているそうです。

このシフトについては、パランティアのビジネスモデルについての解説でも以前書きましたが、AIを上手に活用すればするほど生産性が上がる今の時代、「時間・人数ベースの料金モデル」は時代遅れで、「成果ベース料金」にシフトするのが当然、という話です。

これまで、日本の大手IT企業は、下請け・孫請けを活用するITゼネコン・ビジネスモデルで、官公庁・銀行・JR・通信会社などから莫大な売り上げを上げて来ましたが、今後、パランティアやマッキンゼーが、AIを活用した「成果報酬型」のビジネスモデルで市場に参入してくると、その「甘い汁」が吸えなくなってしまいます。

この動きは、コンサルティング会社だけでなく、多くの専門サービス業（監査、財務アドバイザリー、ITコンサルなど）にも波及する可能性があるため、料金モデルの見直し／契約形態の変化という観点でも注目しておくべきテーマです。

Why most enterprise AI projects fail ― and the patterns that actually work

「どうして企業向けのAIプロジェクトは失敗するのかーそして、成功するパターン」というタイトルの記事です。

この記事によると、

2025年には、企業の約 42%が AI プロジェクトの大半を断念している。2024年の17%から大幅増加している
多くの AI プロジェクトは「技術的なモデルの性能」ではなく、ビジネスとの整合性・運用統合・実装フローの問題で挫折している
とのことです。

失敗する典型的なパターンとしては、以下のような例を紹介しています。

パイロット段階で止まってしまい、本番運用に至らない。モデルは動いても実運用の統合・認証・ワークフロー化が進まない。
モデル最適化（F1スコア等）ばかりに注力し、現場の運用やインフラ・ガバナンスが後回しにされる。
組織内で「技術チーム」「データチーム」「プロダクトチーム」「コンプライアンス部門」が分断され、責任・成功指標・タイミングが共有されていない。
素晴らしいモデルが出来ていても、現場ユーザーの信頼・導入意欲・ワークフロー適合性が欠けていて使われない。
部門やチームごとに「シャドーIT／無秩序なAI実装」が多発し、費用・統制・データ品質の混乱を招いている。
失敗を回避し成功に至る企業に共通するパターンとしては、以下の４つを挙げています。

「ビジネスの痛み（痛点）」を解決対象に設定する
明確に測定可能な成果を設定する
運用・統合を計画に含める
組織・ガバナンス・データ基盤を整える
上のマッキンゼーの話とも通じますが、DXと同じで「ビジネスへのAIの導入」などの漠然としたテーマで、ITコンサルタントを人月工数で雇ってもお金の無駄になるだけです。そうではなく、不良品率を下げる、在庫を減らして利益率を上げる、など測定可能な成果目標を設定した上で、成果報酬型で契約を結ぶべき時代なのです。

AIを使いこなせる優秀なエンジニアをプロジェクトにアサインできないコンサルティング企業は成果報酬型を嫌がるので、それが良いフィルターになります。

The Creditor’s Revolt: How Japan’s Bond Market Just Ended the Era of Free Money and Triggered the Greatest Capital Repatriation in Financial History

先週、日本の10年国債利回りが1.71％まで上がりました。これは一言で言うと、「日本政府が10年お金を借りるときの金利が、リーマンショック前と同じくらいまで戻ってきた」ということを意味します。

ここ30年以上、日本はデフレと超低金利が続き、「世界でもっともお金を安く借りられる国」でした。その結果どうなったかというと、日本の年金・保険・銀行などの大口投資家は、日本国内ではほとんど利回りが取れないそのため、米国債や欧州債など 海外の債券を大量に買う ことで、少しでも利回りを取りに行っていたという構図です。

この「日本マネーの海外流出」が、結果的に世界の金利を不自然なまでに押し下げていたというのがこの記事の主張です。

なぜ「1.7％」程度の金利上昇が大ごとなのか

ところがここに来て、

日銀がマイナス金利をやめ、政策金利を引き上げ
政府が半導体やAI向けの大型の景気対策を打ち出し
市場が「日本は今後も少しずつ利上げしていく」と見始めた
ことで、日本の10年国債の利回りがじわじわと上がってきました。

こうなると、日本の大口投資家から見ると、「わざわざ為替リスクを取ってまで米国債を買わなくても、日本国債を持っていた方がいいかもしれない」という計算になってきます。

結果として、これまで海外に流れていた日本マネーが、少しずつ 日本国内の債券に戻り始める流れが生まれます。

記事が「歴史的な転換点」と表現しているのは、この部分です。

それが世界にどう効いてくるのか

日本マネーが海外から引き上げられていくと、米国や欧州から見るとこうなります。「これまで日本が安く貸してくれていたお金が、だんだん借りられなくなっていく」

すると、中央銀行が政策金利を下げても市中の金利（住宅ローン金利・社債利回り・長期国債利回りなど）は思ったほど下がらないという状況が起こりえます。

もう少し直感的に言えば、「これまで世界は、日本のおかげで“安い金利”に甘えてこられたが、そのボーナスタイムが終わりつつある」という話です。

この流れは、将来の成長をあてにした グロース株（特に未上場AIスタートアップなど）レバレッジをかけて投資しているプレーヤー円キャリートレード（安い円を借りてドル資産を買う戦略）などにとっては厳しい環境になりえます。

記事の筆者は、「40年続いた“タダ同然の金利”の時代が終わり、企業の価値が、よりシビアに“実際のキャッシュフロー”で測られる時代に戻る」 s可能性を指摘しています。

Just added Tesla's A15, Google's new TPU 7 and Mythic's next analog ASIC to the Moore's Law chart.

「ムーアの法則」を、最新のTeslaのA15(次世代FSDチップ)やGoogleのTPU 7を含めてプロットしたグラフです。「そろそろ限界が来た」と言われてから数年が経ちますが、いまだに進化を続けているところが素晴らしいと思います。最近は、NVIDIAのGPUと対抗する高性能なASICが作られていることが良く分かります。



I’ve been thinking a lot about what the net benefit of the AI platform wave is.

MicrosoftのCEO、サティア・ナデラ自身による、AIと企業、AIと社会に関するX上での発言です。要点を箇条書きにすると以下のようになります。

企業にとって大切なことは、「AIをどう自分の力にできるか」であり、自らのAIネイティブ能力を構築し、企業価値を高めることが重要。逆に懸念すべきは、自社の独自価値をテック企業に“移転”してしまうこと。 2.「プラットフォームとは、それを使う人々の経済価値の総和が、そのプラットフォームを作った企業の価値を上回るときに初めて成立する」(ビル・ゲイツの言葉)。つまり、真のプラットフォームは**「使う側のほうがより大きな利益を得る」構造**を持つ。
OpenAIとの関係は、Microsoftの投資がOpenAIのスケールを助け、OpenAIの研究成果がMicrosoftのイノベーションを加速している。両者の強力により、AIスーパーコンピュータ（AIスーパーファクトリー）を共同設計している。
AIによって“カテゴリーそのもの”が拡張する。AIは産業構造を再定義し、市場全体のパイを拡大する「正の和」ドライバーだ。
AI時代の真価は、テック企業の時価総額更新ではなく、「社会全体の進歩」にある。
ゼロサム思考”からの脱却し、各社が自らの領域で成功できるAI活用の仕組みを整えるべき。
AI時代は「より大きな夢と野心」を可能にする。AIは単に業務効率化のツールではなく、社会全体の野心の再定義を促す技術だ。
Oracle is already underwater on its ‘astonishing’ $300bn OpenAI deal

Oracleの株価が、OpenAIとの$300billionの契約を結んだ後に大きく下がっている理由を解説するファイナンシャル・タイムズの記事です。

要点を箇条書きにすると

投資家は「負債を積み上げた大規模データセンター賭け」に不安を感じている。
Oracleは 借金で巨大データセンター（Stargate）を建設し、OpenAIの需要に賭けている。
Oracleは他のハイパースケーラーと違い「土地を持たず」テナント型なので設備投資を前倒しで背負う構造。
2027年以降は 売上の過半がOpenAI関連になる見通し。
Oracleの 純負債はすでにEBITDAの2.5倍（2021年比で倍増）。
キャッシュフローは5年間連続でマイナスになる見込み。
つまり、未上場でまだ赤字を垂れ流しているOpenAIのリスクをOracle株主が被っている構造になっているのです。

Oracleに関しては、私も「AI特需」という観点から評価をしてみましたが、Oracle自身がAIの技術を持っていたり、独自のAIサービスで大きな売り上げを上げている会社ではないので、株に手を出す気にはなれませんでした。同じことは、ネオ・クラウドと呼ばれるCoreWeaveやNebiusに関しても感じており、単にGPUを大量にNVIDIAから購入してAIデータセンターを構築するだけのビジネスに、どうしても魅力を感じることができないのです。

多くの人たちが、「AIバブル」の心配をしていますが、実際にバブルが弾けた時にババを引くのは、莫大な借金を抱えて「AI特需」のキャッシュフローだけに支えられている企業群だと私は思うのです。

質問コーナー

【質問】

OpenAIのApps SDKに関心があり、ちょくちょく触ってます！そこで中島さんの意見も聞きたいと思ってます！

個人開発の立場でも、Apps SDKを使って試作や模倣を通じて学ぶのはいいのですがまずは大手企業が展開する事例を観察し、後からそれを改善し追随する形のほうが合理的だとお考えですか？

UFJもこれを使ったサービスを展開するとの事ですが個人では使われるアプリにするのは難しいかな？と考えたりしてます

《回答》

特に日本の大手企業は、経営陣の考え方が古いので行動が遅く、保守的で、彼らの後をついて行くだけでは、世界の最先端を走ることは難しいと思います。もちろん、ビジネス戦略の一つとして、彼らが作っている商品やサービスの改善を提案するのは悪くないと思います。

ちなみに、企業のAIの使い方は、大きく分けて三つあります。一つは、顧客とのやり取り（カスタマー・サービス）に使うアプローチで、これは技術的に比較的容易なこともあり、導入はさまざまなところで進んでいます。しかし、AIが勝手に値引きの約束をしてしまったり、などの事故も起こっているので、注意が必要です。

もう一つは、AIを活用した業務の効率化で、実際に生み出す価値は、こちらの方が桁違いに大きいものになります。このアプローチは、「従業員一人当たりの生産性を上げる」と言えば聞こえが良いですが、裏を返せば「これよりもはるかに少ない人数で業務をこなせる」なので、日本では大企業が行うと批判される「リストラ」とセットでなければ意味がありません。

三番目は、AIを活用することにより、在庫を減らす、不良品率を減らす、サプライチェーンを最適化するなどして、ビジネスの利益率を上げることです。エンタープライズ向けのAIビジネスとして注目されているパランティアが得意とする領域です。二番目のアプローチと比べると、リストラという痛みを伴わないし、成果（利益率の向上）が良く見えるため、実力のあるITコンサルタントにとっては、狙い目のビジネスです。

【質問】

現在、プロダクトオーナーとして、経理向けの法人支出管理システム(企業における請求書処理や経費精算等をするシステム)を担当しています。

各社、仕訳や経費申請、承認などにAIを活用した機能をリリースをしています。 AIの活用で開発スピードも格段に速くなり、このままいくとAI機能が基本機能となり、コモディティ化していくと考えています。

その中で、プロダクトとして独自性を出していくにはどうしたらいいと考えますでしょうか？

また、経理の人の性格上、「全部自分の目でしっかり見て処理したい」という人が多く、そういう方向けにAIを活用が最低限のレガシーなシステムが残る可能性はあるのでしょうか？※AI機能が当たり前の世の中で、現運用を再現できるレガシーなシステムという独自性、ポジショニングはありえるのか？

《回答》

「導入しやすさ」は重要な差別化要因になりうるので、まずは「どんなユーザー体験であれば、既存の業務プロセスに組み込み易いか」という点にプライオリティを置いてプロダクト開発をするのは悪くないアプローチだと思います。ただ、その際に、「AIを使ってレガシー・システムを操作させる」みたいな短絡的なシステムは作らず、「レガシー・システムに慣れた人が抵抗なく使えるシステムとはどうあるべきか」という観点から実際に触ってもらいながら新しいシステムを設計するのが良いと思います。

【質問】

現在はClaude CodeとCodexの2本で開発を進められているとのことですが、それぞれどのように使い分けられているのでしょうか？

精度があまり良くないと感じた場合、まずプロンプトを見直すのではなく、LLM自体を切り替えてみて、それでも改善しない場合にプロンプトを調整する、というような運用をされているのでしょうか？

《回答》

基本的には、Claude Codeを使い、それでうまく行かなかった場合に、Codexを使っています。とは言え、9割型はClaude Codeで思ったように開発できるので、Codexを使うケースは稀だし、なくてもそれほど困らないと思います。

【質問】

AIを使っている時に少し疑問を持ちましたので質問させてください

ChatGPTを使いジムでのトレーニングについて体組成計のデータとトレーニング内容を都度送り、身体の状況にあったトレーニングを教えてもらうという使い方をしています。

しかし2週間ほど経った時点で、AI側の回答の時系列のズレが生まれました。

例えば、昨日や一昨日といった表現をするとその時系列を理解できず両方とも昨日のように捉えるのです。

AIに聞いてみるとAI自体は受送信の記録をもっておらず、日付や時間の管理ができないとのことでした。対策としてはDay1やDay2といった形でのやり取りで解決はするそうですがその手間が少し嫌だなと感じています。

AIは日付や時間といった概念は持たないように設計されているのでしょうか？

《回答》

ChatGPTに使われているAI（GPT-x）そのものには記憶機能がなく、OpenAIは、ChatGPTというアプリに「メモリー」と呼ばれる記憶機能を追加してあります。実際にどんな仕組みで実装されているかは公開されていませんが、時系列などの把握には力を入れていないと私は理解しています。

【質問】

中島さんはNBIS（Nebius Group NV）をどのように評価されておりますでしょうか？NBIS関しての今後の企業価値、ポテンシャルの考察をお願いしたいです。よろしくお願いします。

《回答》

別のところにも書きましたが、AIデータセンターを運営するだけのビジネスには魅力を感じないし、「評価のしようがない」と感じています。このビジネスは技術力ではなく、「NVIDIAから十分な数のGPUを入手できるか」「新しく作るデータセンターに十分な電力を確保できるか」などで決まるため、その点について、私が評価するのは不可能だと感じています。

ちなみに、「McKinsey just published a very interesting article on the Neoclouds sector.」というXの投稿には、Nebeiuなどのネオクラウド・ビジネスに対する評価が紹介されていますが、ネオクラウド・ビジネスが立ち上がったのは、単に一時的な「GPU不足」の波に乗っただけの話で、単にGPUをレンタルするだけのビジネスはコモディティ化するので、特定業務向けの推論など付加価値を生み出すソフトウェア・スタックによる差別化を行う必要があると書かれています。

Nebiusが中長期的に価値を生み出す会社になるかどうかの判断は、その辺りに関する経営陣が語るビジョン、戦略、実績を見て判断するのが良いと思います。

【質問】

中島さんはご自身で株式や不動産などを選び投資されていますが、資産運用会社やプライベートバンクなどを通して資産を運用しようとは思わないのでしょうか？やはり自分で運用した方がパフォーマンスが良い自信があるのでしょうか？

《回答》

一部の資産は、（特定のサービスを受けるために）プライベートバンクに運用を委託していますが、大半の資産は自分でマネージしています。単に市場全体の波に乗りたいのであれば、手数料の安いETFが一番だし、個別株を買うのであれば、自分で選ぶべきだ、というのが私の考えです。

【質問】

大手メーカーの研究開発部門でライン管理職を務める者です。中島さんは「尖った人材」についてどう考えますか？

しばしば「他人に負けない尖った才能があればよい」ということを聞くと思いますが、ある一点に特化していてもそれ以外のことができないと、”正直扱いにくいなぁ”というのが私の感想ではあります。というのも、その人の得意な業務にアサインしても、会社の業務はそれ1つだけで成り立っているわけでないので、他の業務領域もやらせることになり、途端にパフォーマンスが落ちる傾向があるためです。 (例えば、製品開発自体はまあまあできても、資料の作り方が下手で人(顧客)に伝えるのが下手。社内雑務も組織上分担させなければならず、分担したはいいがまともに機能しない)
同じようなことに悩んでいるライン管理者も多いと思いますが、何かしらアドバイスいただければ幸いです。

《回答》

尖った人材でも上手な使い方をすれば、有効に使えると思います。問題は、「会社の業務はそれ1つだけで成り立っているわけでないので、他の業務領域もやらせることになり」という部分にあるように思います。そんなカルチャーやシステムがある会社と尖った人材の相性は良くありません。

【質問】

11月18日号で言及されていた「ライター稼業」について質問です。

私は現在BtoB領域のWebコンテンツを執筆する仕事をしています。対象とする読者層は企業の購買担当者、管理職、役員などです。

BtoBですと多くの場合、詳細な資料を入手するには社名・メールアドレスなどを入力してホワイトペーパーやカタログをダウンロードする必要があるため、AIが学習データを収集しにくい面があるように思います。商談に持ち込むために多くの企業があえて情報を公開していないわけですが、この分野も近い将来ライター稼業は厳しくなっていくでしょうか。

コンテンツマーケティングだけでなく独自性の高いコラム執筆も手がけたほうが良いなど、アドバイスがありましたらご教示いただけますと幸いです。（補足：メルマガではビジネスにならないというお話でしたが、現状では年間1,000万円以上の原稿料を安定的に得ております。）

《回答》

「年間1,000万円以上の原稿料を安定的に得ている」であれば問題ないと思いますが、いわゆる「ライター稼業」をしている人たちの何パーセントぐらいの人がそのレベルの収入を得ることが出来ているのかに興味があります。私の場合には、たまたまメルマガに多数の読者がいますが、それがなければ年間1,000万円を稼ぐことは、ほぼ不可能です（「あなたの仕事はなぜ終わらないのか」クラスのベストセラーを毎年出版する必要があります）。

「BtoB領域のWebコンテンツ」を執筆する作業が、現時点では学習データ不足でAIにはできないとの指摘ですが、事前学習ではなく、プロンプトの一部として資料を与えて文章を書かせることも十分に可能なので、その領域のビジネスもAIに（正確にはAIを使いこなす人に）侵食されてしまう可能性は十分にあると思います。

私であれば、まずは、自分自身がAIを活用して生産性を高めることにより、短期的には収入を増やしつつ、(AIによる)単価の下落に備えるあたりからスタートするだろうと思います。

【質問】

OpenAIの新しいブラウザ環境「Atlas」を試されたことはありますか？もし既に使われていれば、使用感や評価をお聞きしたいです。

私自身は、従来よりも低コストでブラウザ操作系のワークフローを実現できそうだと感じつつも、まだ細かな操作性に改善の余地があるように思っています。

《回答》

試そうとは思っていますが、まだ試していません。それよりもまず先に、自分自身でAI機能が組み込まれたブラウザーを作ってみたいと考えていますが、まだ手を動かせていません。ちなみに、ブラウザーは、銀行口座にアクセスしたり、クレジットカードで買い物をしたりするツールなので、AIの不具合で情報が漏洩したり、勝手に買い物をしてしまったりなどのリスクもあるので、慎重に導入することをお勧めします。

【質問】

私の10歳の息子についてご相談したく、質問を送らせていただきます。

息子は小さい頃からADHD的な特性があり、集団生活は問題なく過ごせている一方で、「人の話を聞く・文字を読む」が苦手です。勉強も継続的に取り組むことが難しく、特に算数の文章題になると集中が切れてしまいます。

一方で、機械や電子機器、ものづくりへの興味と没頭力は飛び抜けており, 幼い頃からスマホやゲーム機を分解・修理したり、友達の自転車を直して親御さんに感謝されたり、祖母のスマホのロックをドコモ店員でも解除できなかった状態から直したこともあります。

ロボット教室にも通いましたが、既に決まった手順通りに作るのがつまらなくなり、2年ほどでやめてしまいました。自分のアイデアで何か生み出すのは興味がないようでした。

現在は小1から続けてきたバスケットボールで頭角を現し、体格にも恵まれ「期待の選手」として認められつつあります。

しかし、学習面は相変わらず伸ばし方がわからず、私も必要以上に口を出してしまい、うまく関わることができていないと悩んでいます。真ん中長男で自己肯定感が低いなとも感じていますが、褒めれば伸びるタイプではあります。素直さと柔軟性があればなぁと思います。

ただ、息子には

何時間でも没頭できる集中力
直感的に構造を理解する力
電子機器・修理・ゲーム攻略の異常な早さ
といった強みがあります。

中島さんなら、このようなタイプの子どもが持つ資質を、どのように育て・伸ばしていきますか？テック領域で生きていく可能性はあるでしょうか？また、親としてどんな環境や機会を用意してあげるべきでしょうか。ぜひ中島さんの視点でアドバイスをいただければ幸いです。

《回答》

大切なお子さんのことなので、無責任なことは言えませんが、人間のタイプを10個ぐらいに分けるとしたら、お子さんはエンジニア向けの気質を持っていると言えます。私も子供の頃は、一つのことを何時間もしていたために、親に心配をかけたことがあるぐらいです。

親から押し付けてもうまく行かないので、色々な機会をお子さんに与えて、その中から夢中になれるものを見つけてもらうのが一番良いと思います。私の場合、学研の「科学」という月刊雑誌を親が私に買い与えてくれ、それに付録としてついてくる様々な実験材料に夢中になっていたことを良く覚えています。結果として、科学全般が大好きで得意になり、それが今の自分のベースになっています。

【質問】

Microsoft Windows部門に着任した方のエージェント型OSに関するポストについてです。

https://x.com/pavandavuluri/status/1987942909635854336?s=20

Windowsのエージェント型OSへの進化について、中島様はどう捉えますか。

あくまでもWindowsOSという旧来からのOSをベースとし、OSNativeなレベルでエージェントが付加されるようなイメージかと思いました。 AINativeという指向とはだいぶ方向性は異なるとは思いつつ、PCというハードウェアに縛られたり、Windowsというベースである以上、これ以上のイノベーションが起きるのはなかなか難しいのかなと私は思いました。

《回答》

「エージェント型OS」の定義が曖昧ですが、「エージェント＝自律的にタスクを実行してくれるソフトウェア」という定義に基けば、様々なタスクをインタラクティブではなく、バックグラウンドで並列実行してくれるOSというイメージになります。

OSがエージェント機能を持つことは悪くないと思いますが、「エージェント型OS」と呼ぶのにはとても違和感があります。ちなみに、私の中では、AIネイティブなOSは、MulmoChatで示しているように、チャットとGUIを融合した結果、ユーザーが「アプリケーション」を意識せずに様々なタスクをAIの助けを借りて行えるような形であるべきだと考えています。

【質問】

プログラミングではCursorではなくClaudeを利用されているということですが、その理由を教えていただけますでしょうか？CursorでもClaude Sonnet4.5を利用できます。例えば性能面やコスト、使い勝手など、ご経験に基づいたポイントをお聞かせいただけると大変参考になります。

《回答》

私が、Claude Codeを気に入っているのは、単にユーザー体験の違いによるものです。Cursorの場合、AIとのチャットがIDEそのものに組み入れられているため、自分が行った変更と、AIが行った変更の境が曖昧になり、混乱してしまうことが何度もありました。

Claude Codeを使う際には、「これからはAIにコードを書かせる時間」と決めた上で、IDEの外から（実際には、IDEの中で動くTerminalアプリから）コードに変更を加えるため、私のbranchを多用し、こまめにcommitするコーディングスタイルと、とても良くマッチしているのです。

たぶん、Cursorを使っても同等のことはできるのでしょうが、一度Claude Codeに慣れてしまうと、あえてCursorのChatの使い方を学ぶ気にはなれないのです。

【質問】

中島さんはどのくらいの頻度で散髪されていますか？

ヘアスタイルからすると、2～3週間で整えないと重たくなりそうな印象があります。

私も似た髪型で、2週間おきに切っています。ただ、徒歩7分のサロンへ行くのも面倒で、「髪が伸びなければいいのに」と思うほどです。サロンでの会話も得意ではなく、自分にとっては単純作業のように感じています。短髪男性の方には同じ悩みが多いのではないでしょうか。

ここからテクノロジーに関する質問ですが、人型ロボットの研究が進む中、「髪を切れるロボット」はいつ頃実現可能でしょうか？

歩行機能は不要で、ロボットアームのような形でも構いません。美容師にセットしてもらった髪型を学習し、いつでも同じスタイルを再現できるロボットを自宅に置きたいです。

ホテルにもあれば出張時にも便利ですし、購入が難しい人には1000円カットのQBハウスのような低価格帯の代替にもなり得るかもしれません (スマホからデータ連携)。既存の美容室は会話や接客を楽しみたい人向けの高級サービスとして残り、二極化する未来も考えられます。

こうした未来は実現し得るでしょうか？

もし可能であれば、実現まであとどのくらいかかると思われますか。

《回答》

私も散髪は３～４週に一度の頻度ですが、シアトル、ハワイ、東京の三拠点で活動しているため、どのタイミングでどこで散髪するかは毎回悩みます。一番気に入っているのは、東京の家の近くにある美容院ですが、そこに毎回行くこともできないので、米国でも散髪をしていますが、いまだに気に入った床屋・美容院は見つかっていません。日本人の固い髪の扱い方を知らないのが一番の問題です。

散髪の自動化は是非とも実現して欲しいと思います。自動化の一番のメリットは、再現性の高さで、一度気に入った髪型にしてもらえたら、それを覚えてもらって、次からはそれを再現してもらうだけです。この仕組みは、チェーンやフランチャイズ・ビジネスも相性が良く、大きなビジネスになる予感がします。

【質問】

OECDの調査で、日本人の睡眠時間は7時間22分と加盟国33ヶ国のなかでもっとも短いというデータがあります。一方、私の人生で会った人たちの平均はせいぜい6時間くらいの感覚です。特に社会人になってからは7時間寝ている人はほとんど見ません（特に、優秀な人たちは）。

私の周りが短すぎるだけでしょうか？　中島さんは普段何時間くらい寝ていますか？　ビル・ゲイツやジェフ・ベゾスなどの起業家もよく寝るようですが、中島さんの周りの優秀な方は、皆睡眠を大事にしている感じでしょうか？

《回答》

日によって異なりますが、平均したら7時間半～8時間ぐらい寝ていると思います。人によりますが、しっかりと睡眠をとって、すっきりとした頭で仕事をするのが一番だと私は思います。「早寝、早起き、腹八分」が私の健康法です。

【質問】

Googleがリリースした「File Search Tool」に関する記事を拝読しました。率直に、とても便利な仕組みだと感じました。

私の勤め先では、日々のドキュメントをどのサーバーディレクトリに保存するかで、毎回悩みが生じています。「どこに置けば後から探しやすいか」という点が常に課題です。しかし、このツールでは ドキュメントをただ放り込むだけで必要な情報が検索できる──そんな世界が実現していて、まさに理想的だと思いました。

この流れでいけば、あと1年もしないうちに、私たちの環境にも同様の仕組みが自然と実装されていくのではないか──そんな期待も膨らんできました。

《回答》

Microsoftは昔から、そんな機能をWindowsそのものに持たせようとする試みをして来ましたが、失敗の連続でした。そろそろそれが可能になっても不思議はないと思います。

【質問】

中島さんは、ご自身のポートフォリオの中で、コアとサテライトの比率をどの程度にされていますか？これまでのご発信内容から拝見すると、コア部分はあまり重視されていない印象を受けました。

また、投資においては分散も重要だと思いますが、中島さんの保有銘柄はテクノロジー分野に比較的集中していると感じております。このあたりのリスクの取り方やお考えについても、ご教示いただけますと幸いです。

《回答》

「コアとサテライト」という考え方とは少し異なりますが、Apple、Microsoftのように安定した利益を上げ、PEも比較的低い企業群と、パランティアのように成長過程にあり、高いPEを持つ企業群、という意識は持っています。別の言い方をすれば、ハイテク株の中で、「コアとサテライト」と同等のイメージを持って資産配分をしていると言えます。

【質問】

以前のメルマガQAで、今後上場された場合の、Anthropicの株の購入（上場後少し購入＋定額）を示唆されていましたが、現段階の未公開株の状態での、ファンドへの投資等は考えられてはいませんでしょうか？

もし考えられている or 考えられていない場合、その理由も合わせて教えて頂けると嬉しいです。（ご参考までに、下記URLを貼付いたします） https://prtimes.jp/main/html/rd/p/000000062.000035674.html

《回答》

考えたこともありますが、手数料が高いだけでなく、ファンド側はリスクフリー、リスクは100％投資家が負うという構造が気に入らないので、避けています。

【質問】

私は機械系・電気系の現場エンジニアをしております。

以前から「ハード側のエンジニアでもソフトウェアも理解していなければならない」と強く感じています。個人的には、データサイエンスの勉強を足がかりに、ソフトウェア、ひいてはAIなどの勉強を続けています。

そうした背景の中で、中島さんのメルマガを大変勉強になる思いで拝読しており、毎号とても示唆に富んだ内容で、読むたびに視野が広がる感覚があります。

一方で、私にとっては馴染みのない専門用語や概念も多く、ChatGPTなども使いながら一つ一つの用語を調べているうちに、1本読み切るのに3～4時間かかってしまうことも少なくありません。調べて積み重ねた知識は、自分なりにいつか発信したいとぼんやりと考えていました。

そのような中で、先日の MulmoCast をリリースしていただいたのを機に、半月程度使い倒してみましたが、配信コストが一気に下がったと強く感じました。そこで、自分のような方に向けて、動画でかみ砕いて解説するチャンネルを作ってみたいと考えるようになりました。

まだ手始めなので明確に方向性が定まっていませんが、引用させていただきたいことについては、「Life is Beautiful」で取り上げられているトピックやキーワードを題材に、

自分の言葉でかみ砕いて解説する
特に「これからソフトウェアやAIも学んでいきたいハードウェアエンジニア」向けの内容にする
といったイメージです。

メルマガに「引用元を明記すれば引用してよい」という趣旨の記載がありましたので、ルールに従えばそのまま進めることも可能かと思いましたが、継続的に動画の題材として扱わせていただく以上、一度きちんとご相談し、許可を頂いてから進めたいと考え、今回メールをお送りした次第です。

つきましては、もし差し支えなければ、メルマガ「Life is Beautiful」の本文を出典を明記したうえで、上記のような「ハードウェア系エンジニア向け解説動画」の中で引用・要約・解説させていただくことについて、ご承諾いただけますと幸いです。

《回答》

出典さえ明確にしていただければ大丈夫です。特に、MulmoCastを使っていただけるなら大歓迎です。

【質問】

私はスモールビジネス、あるいはスタートアップとして、この拡大する防衛予算の市場に参入したいと考えています。

大手が参入しない「隙間」について 既存の大手企業（ハードウェア主体・意思決定が遅い）が構造的に苦手とし、かつスモールビジネスでも勝機があるのは、具体的に防衛産業のどの分野（ソフトウェア、ドローン、サイバーセキュリティの特定ニッチなど）だと思われますか？

具体的なビジネスアイデア もし中島様が今、資金とリソースを限定された状態でこの分野に参入するとしたら、どのようなアプローチ（ビジネスアイデア）で切り込みますか？

「日本版パランティア」の実現可能性 米国ではパランティア（Palantir）のようなデータ解析企業が国防の中枢に入り込んでいますが、日本でも同様に、特定の大手SIerではなく、新興のテック企業が「日本版パランティア」のようなポジションを築くことは可能だと思われますか？ それとも、日本の既得権益構造では難しいでしょうか？

防衛という特殊かつ規制の厳しい業界に対し、ソフトウェアやAIの視点を持つ中島様ならどうハックされるか、ぜひご意見を伺いたいです。

《回答》

今後、防衛予算が増えることを考えれば悪くない戦略だと思います。特に、ロボット・ドローンなどの技術は、非軍事技術への転用も可能なので、「国の軍事予算を使って、技術開発をし、その技術の転用ビジネスで大きな価値を生み出す」という戦略が取れるので、とても良いと思います。

「日本版パランティア」も悪くないと思いますが、その際には、このメルマガで以前から指摘しているように、人月工数ビジネスモデルではなく、成果報酬ビジネスモデルで攻めることを忘れないでください。人月工数ビジネスモデルで攻める限り、官公庁と強い関係を持つ大手ITゼネコンから仕事を奪うことは簡単ではありません。

【質問】

Gemini 3.0の推論能力を活かす器としてGoogleが投入したAntigravityを使ってみて、中島さんの率直な感想（UX/UIの心地よさ）を伺いたいです。

《回答》

少しだけ使ってみましたが、悪くないと思います。今後、Claude Codeを置き換える存在になるかどうかは、しばらく使ってみないと分かりません。
