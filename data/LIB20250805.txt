週刊Life is beautiful ２０２５年８月５日号：Teslaの半導体戦略、日米貿易交渉の解釈、MulmoCast: サウンド・エフェクトとリップシンク、Claude Code とペアプログラミング


今週のざっくばらん

Teslaの半導体戦略

TeslaからSamsungとの提携のアナウンスがありました(Samsung to Make Tesla AI Chips in Multiyear Texas Deal)。TeslaとSamsungが、Tesla向けの半導体をテキサスで製造する契約を結んだ、というものです。

Teslaの戦略を理解する上で、そして、AIチップ・マーケットの今後を占う上でとても重要な発表なので、詳しく解説します。

発表された契約の内容は以下のようなものです。

Samsungは、テキサス州にTeslaの次世代チップA16だけを作る半導体工場を建設
Teslaは、$16.5billion（約2兆4473億円）分のチップを購入することを約束
契約は2033年まで
Elon Musk自らが半導体工場の生産性の最適化に協力
Teslaは、現在販売されているTesla車に搭載されているHW4向けのAIチップ、A14（7nmプロセス）はSamsungに製造を依頼していますが、次世代のHW5向けのAI5（3nmプロセス）はTSMCに製造を依頼することが決まっています。

AI5の設計は既に完了しており、TSMCによる大量生産が始まるのは2026年であり、販売されるTesla車に搭載されるのもそれ以降になります。AI5の計算能力は、A14の4～5倍の2,000～2,500TOPS（Tera Operations per Second）と予想されています。

参考までに言うと、NVIDIAが今自動車向けに販売しているチップ、Orion（8nmプロセス）は、254TOPSで、次世代のThor（4nmプロセス）が約2,000TOPSと、Teslaのチップとほぼ同じスピードで進化しています。

今回、TeslaがSamsungに製造を依頼するAI6は、さらに、その一つ先の世代のもの（2028年ごろのTesla車に搭載されるHW6向けのもの）であり、Elon Muskの長期ビジョンに基づいた積極的な投資の姿勢をはっきりと見ることが出来ます。

その頃（2028年以降）になれば、ロボタクシーのサービスも本格的に立ち上がっているし、人型ロボットOptimusの大量生産もスタートしているでしょうから、そこに必要なAIチップの生産ラインを今のうちに確保しようとするものです。

最先端技術で世界をリードするTSMCではなく、3nmプロセスの歩留まりの低さで悩むSamsungと契約を結んだ点が意外ですが、

TSMCにはチャイナリスク（中国による、武力を使った台湾の併合）がある
Samsungが、Teslaチップ専用の工場の建設に同意した
Elon Muskが関わることにより、歩留まりの高い半導体工場が作れる可能性がある
長い目で見て、TSMC一社に頼るのは良くない
などの理由からSamsungとの契約に至ったのだと思います。Samsungだけに頼ることにリスクが高いと考えれば、「TSMCとも契約する」可能性も十分にありますが、まずはSamsungを支援し、TSMCの独占状態に歯止めをかけたかったのだと思います。

ちなみに、米国政府は、バイデン政権時代に、$39billionを投じて米国内に半導体製造工場を複数作る「2022 Chips Act and Science Act」を発令し、Samsungはそのうち$4.75billionの補助金と、$9billionの減税措置を受けることが決まっていました。

トランプ政権がバイデン政権時代の政策にブレーキをかけ始めたため、少し雲行きが怪しくなっていたところでしたが、今回、トランプ氏が日本やEUから巨額の資金を引っ張ることに成功したので、再び加速すると見て間違いありません。

特に、日本と結んだ$550billionの投資の約束は米国側にとってとても有利なので（日本側は最大で10％しか株を持てない）、それがTSMCやSamsungの米国内の半導体工場の建設費用に使われる可能性は十分にあると見て良いでしょう。

日米貿易交渉の解釈

日米の貿易交渉の結果については、先週号でコメントを書いたし、YouTubeでも説明しました（日米交渉の裏側。日本だけが背負う「ハイリスク・ローリターン」の構図）。

その後、交渉担当だった赤澤大臣から以下のようなコメントが出ました。

80兆円は、タダで米国に渡すもの（彼らの用語で「真水」）ではなく、投資・融資・債務保証のセット。
当初の日本からの提案は、利益を50:50に分けるものだった。
トランプ氏が利益配分にこだわるため、そこは譲歩して、米国90:日本10で決着した。
80兆円のうち、（利益配分に関わる）投資は1～2％なので、そこを譲歩しても小さな話。
日本は、利息で儲ければ良い。
債務保証は、貸し倒れの際の保証をするだけなので、その場で税金は出て行かない。
米国からも公式な発表があり、多くの人が「日本と米国で言っていることが違う」と指摘していますが、それは間違いです。双方の主張を見る限り、日米それぞれの発言に矛盾はなく、単に、自国の国民に対する説明の仕方が違うだけです。

どんなプロジェクトであれ、巨額なお金を必要とするプロジェクトは、投資と融資を組み合わせます。投資とは株主として経営に関わり、プロジェクトが成功した際には、持分に合わせた利益をもらう権利と紐づいています。一方の融資は、債権者として会社にお金を貸すだけなので、得られるのはあらかじめ決まった利息だけです。

大規模なプロジェクトの場合、投資と融資の比率は、1:9や2:8のように融資の形で調達する方が多くなるのが一般的です。プロジェクトの運営を行う出資者としては、可能な限り、自分たちが提供する資金は抑え、銀行などの金融機関からの融資の割合を増やすことにより、「投資効率」を高くしたいからです。

また、日本の会社が他の国に進出する場合、株を51%以上持ち、資本金だけでは足りない分を融資で補うのが一般的です。株を51%以上持っていれば、株主として経営権を握れるし、事業が成功した場合に、大きな利益を得ることが出来るからです。

株を10％程度持つケースもありますが、その場合、経営には参加せず、単に会社としての関係を築く、もしくは、純粋な投資家として、キャピタルゲインを狙う、などの意味があります。

なので、日本が米国に投資をする際に、50%以上持つのか、10％しか持たないのかでは、意味が大きく変わってきます。

当初、日本側が50:50と提案したのは、米国側と対等な立場でプロジェクトの経営に関わり、事業が成功した際には、大きな利益を得たいから、そう提案したのです。

トランプ氏が日本の株の持分を10％に下げたがったのには以下の二つの意味があります。

プロジェクトの運営には口を出して欲しくない
プロジェクトが生み出す利益の大半を米国が受け取りたい
米国が必要としている、半導体・エネルギー・レアメタル・AIなどのプロジェクトには莫大なお金がかかり、かつ、大きなリスクが伴いますが、もし事業として成功させることが出来れば、何兆円、何十兆円という規模の莫大な利益を生み出す可能性を持っています。GAFAMの後を担う企業がそこから生まれても不思議はありません。

トランプとしては、これらのプロジェクトが生み出す利益を可能な限り米国のものにしたい、経営権も米国側で握っておきたいからこそここにこだわったのだと思いますが、起業経験もない官僚出身の赤澤氏には「株を持つこと」のメリットが感じられず、そこを思いっきり譲歩してしまったのでしょう。

ちなみに、トランプ政権は同様の合意をEUと結びましたが、「利益の９０％を米国のものにする」という条件が入っていない点には注目すべきです。つまり、株の持分に関して、ここまで譲歩する必要はなかったのです。

【追記】韓国との合意にも利益の分配に関する記述は見当たりません。

MulmoCast: サウンド・エフェクトとリップシンク

先週は、MulmoCastにサウンド・エフェクトとリップシンクの機能を追加しました。

サウンド・エフェクトは、動画に適した音を後から追加する生成系AIで、リップシンクは、与えた音声に合わせて、動画中の人物の口と表情を動かして、あたかもその人が喋っているように演出する生成系AIです。

どちらの生成系AIの技術も、まだ発展途上中の技術で、技術も未熟だし、値段も高いので、現時点では、まだ、それほど活用できるとは思えません。しかし、今の進化のスピードを考えれば、１～２年で品質も高くなり値段も下がることは明確なので、今のうちからMulmoCast側で準備しておくことは重要だと考えたのです。

サウンド・エフェクトの方は、以下のような形式でスクリプトにプロンプトを指定します。このケースでは、既に映像として馬が走る姿が描かれているので、特にプロンプトを指定しなくても適切な音が生成されます。

  {
    "duration": 10,
    "imagePrompt": "A wide view of a horse galloping fast in a large field",
    "moviePrompt": "A horse galloping fast in a large field",
    "soundEffectPrompt": "galloping"
  },
リップシンクの方は、"text"で話す内容を指定し、"moviePrompt"で話すべき人の映像を生成した上で、"enableLipSync"フラグをオンにするだけです。

  {
    "text": "In the future, advancements in AI could revolutionize industries like healthcare, education, and transportation.",
    "moviePrompt": "A female presenter in a futuristic cityscape with AI-powered hospitals, autonomous vehicles, and interactive learning environments",
    "enableLipSync": true
  },
Claude Code とペアプログラミング

少し前にも書きましたが、AIを使ったコーディング手法は色々と試しているものの、現時点では、Cursorのオートコンプリートと、Claude Codeの組み合わせが一番気に入っています。

上に書いたサウンド・エフェクトの場合、以下のような手順でClaudeを活用しました。

何をしようとしているか説明した上で、まずはMulmoScriptのスキーマを変更するように指示（追加するプロパティの名前とデータタイプは、具体的に指定）
サウンド・エフェクトのAPIを呼ぶエージェントを、映像生成のAPIを呼ぶエージェントのソースコードを生成するように依頼。
作ったエージェントを呼ぶために必要な構造体の追加を依頼
graphAIからこのエージェントを呼ぶ際に必要なパラメータを渡すコードの生成を依頼
実際にこのエージェントを呼び出すコードの追加を依頼
テスト用のサンプルスクリプトの作成を依頼
ここで一旦テスト。問題が見つかったので、修正を依頼。
最後に一つの映像を作る際に、サウンド・エフェクトが追加された映像を使うように指示
さらにテスト。不具合が見つかったので、ここは手動で修正
２時間ぐらいの作業でしたが、まるでキーボードを正確に打つのが得意なジュニア・エンジニアとペア・プログラミングをしているような体験でした。設計は私が行い、コードの変更はステップ・バイ・ステップで行わせたため、（AIに全て任せてしまった時のような）手戻りもなく、非常に効率良く開発が出来ました。

特に、既存のエージェントをテンプレートとして新しいエージェントを作る部分などは、機械的な作業でありながら、手間もかかるし、人間がやると些細な間違いをしてそこで時間を取られるので、そこをAIに任せることが出来るのがとても便利です。また、複数のファイルにまたがった変更やリファクタリングなども、AIはとても効率良く処理してくれるので、助かっています。

細かな話ですが、こんなスタイルで作業する際には、（gitの）ブランチを作った上で、ステップ・バイ・ステップでAIにコードを書かせながら、毎回、変更をそのブランチに小まめにcommitするスタイルがお勧めです。コミットの粒度を小さくしておくと、小さなステップで戻すことが容易なので、AIが書いたコードがうまく動かない時などに、躊躇なく捨ててやり直すことが出来ます。

私の目に止まった記事

First principles building design is absolutely critical for scaling human civilization.

新型コロナのワクチンに関しては、「ビルゲイツがマイクロチップを人々の体に植え付けるためにやっている」という陰謀論から、「何もしなかったテキサス州には、パンデミックは起こらなかった」まで様々な情報が飛び交い、何が正しいのかが分からなくなっているのが現状です。

ワクチンは特定の種類の伝染病にはとても有効ですが、新型コロナのような伝染性が高く、かつ、重症率の低い伝染病に対して、果たしてワクチンや患者の隔離が本当に必要だったのかは、専門家たちの間でしっかりと検証した上で、次のパンデミックに備えて欲しいと思います。

そんな中で、このXの投稿はとても新鮮でした。ワクチンよりもはるかに有効な感染予防の手法があるという主張です。

結論から先に言えば、建物の換気システムのアップグレードで、オフィスやレストランなど複数の人が集まる場所においては、無駄に空気をかき混ぜる今の換気方式から、半導体工場で採用されているような空気の流れをコントロールする形の換気方式により、空気感染の可能性を大きく減らすことが出来る、という主張です。

空気を流すダクトの設計からやる必要があるため、既存のビルに適用するのはコストが高すぎて現実的ではありませんが、新築のビルに関しては十分に可能な話であり、建築基準法（もしくはそれに類推するもの）を改訂して義務付けることにより、数十年かけて「空気感染の起こりにくい社会」を作ることには大きな価値があると思います。

ちなみに、この記事の中に、「通常のビルの中では、肺に取り込まれる空気の４０分の１は他人が吐き出した息だ」という表現があり、それだけでは、「そんなものか」としか感じませんでしたが、「これは４０回に１回、他人の吐き出した息を吸っているようなもの」という表現にはハッとさせられました。同じことを語っているはずなのに、表現だけでこれだけインパクトが違うというのは興味深い現象です。

Arm-Backed Chipmaker Ambiq Raises $96 Million in IPO

先週の水曜日、Ambiqというファブレスの半導体の会社が上場しました（参照：S-1）。これまで目にしたことが無い会社だったので、少し時間をかけて調査してみました。特徴を箇条書きにすると、

創設者はScott Hanson。ミシガン大学で電子工学の博士号を取得（2008年）した後、そこで研究者として2年間働いた後に、Ambiqを設立（2010年）。当初はCEO。
今のCEOは江坂文秀（2015年より）。その前は、TransphormのCEO（2013～2015年）。Transphormは、窒化ガリウム半導体（GaN半導体）の会社で、2024年にルネサスに買収されている。
主力製品は、低消費電力のARMベースのSoC（System On Chip）。Fitbit(Googleの一部)、Garmin、Huawei、Xiaomiなどのウェアラブル・デバイス（スマートウォッチなど）に採用されている。
AmbiqのSoCは、SPOT（Sub-threshold Power Optimized Technology）と呼ばれる、通常のCMOSトランジスタよりも低電圧で動かすことにより、低消費電力（50%～90%減）でARMベースのチップを動作させることが出来る。
2024年の売り上げは$76.1M、粗利率は31.9%でしたが、$39.6Mの最終赤字でした。
面白いポジションにある会社だと思いますが、まだ赤字の会社であり、キャッシュフローがプラスに転じる前に現金の底がついてしまう可能性があるリスクの高い会社です。ただし、その分、株価総額も安いので、伸び代が大きい会社とも言えます。

IPO価格は$24でしたが、初値は$40を超えてしまいました。少し割高にも感じますが、モニタリングするために、少しだけ株を購入しました。

Vibe code is legacy code

「バイブ・コードはレガシー・コードだ」と指摘する、含蓄のあるブログ記事です。「レガシー・コード」という言葉は使っていませんでしたが、私も全く同様のことを感じていたので、共有します。

バイブ・コードとは、コーディングをAIに任せて、自分ではコードを確認もせずにソフトウェアを作ってしまうことで、ソフトウェア・エンジニアでない人も、それなりに動くアプリケーションやウェブサイトを作ってしまえることで注目されています。

一方のレガシー・コードとは、一般的には、以前に会社にいた誰かによって書かれたコードですが、その人がいなくなってしまって以来、メンテナンスもされておらず、誰も会社に理解する人がいないコードのことです。どの会社にもよくある話ですが、下手に修正しようとすると予想もしないバグが発生することもあるため、会社にとっては、「負の資産」とも言える存在です。

このブログの筆者は、コードレビューもせずにバイブ・コーディングで作ってしまったコードは、誕生した瞬間から負の資産とも言える、レガシー・コードだと指摘しています。プロトタイプや使い捨てのコードには十分ですが、生成したコードを長く使おうとする場合には、バイブ・コーディングは適さない、と指摘しているのです。

A comprehensive list of 2025 tech layoffs

Microsoftが、業績が上向きなのにも関わらず大量の従業員をレイオフ（解雇）したことが注目されていますが、それはMicrosoftに限った話ではなく、テクノロジー業界全般に渡るものです。この記事は、2025年前半に各社で行われたレイオフを集計しています。

一連のレイオフは全てAIによるものとは言えませんが、人間並みに（もしくは人間以上に）賢いAIが誕生した今、私たちの働き方は大きく変わろうとしており、その大きなシフトの最先端にいるのが、テクノロジー業界の会社たちだと解釈して思います。

AIの力を借りることにより、従業員一人当たりの生産性を大きく上昇させることができれば、より少ない人数でより多くの売り上げ・利益をあげることが可能だと考えるのは、株主利益を最大化する責任を持つ経営者として当然です。特に最近は、最初から賢いAIの存在を前提として設計されたAIネイティブな会社が数多く誕生しているので、業界で圧倒的な力を持つMicrosoftですら、これまで通りの働き方では戦えないのです。

Period	Layoffs (est.)	Notes / Context
January 2025	~2,400	Early cost-cutting wave across tech
February 2025	~16,234	Peak wave of layoffs industry-wide
April 2025	~24,500	Major restructuring efforts
May 2025	~10,397	Ongoing right-sizing measures
June 2025	~1,606	Smaller adjustment period
July 2025 (to date)	~16,142	Mix of AI-driven cuts, startup shutdowns
Company	Layoffs (est.)	Notes / Context
Intel	~24,000 (by year-end)	25-31% of workforce; plant closures in Germany, Poland, Costa Rica
Meta	~3,600 (Jan)	5% of staff; performance-based cuts
Microsoft	~6,000-9,000	Cloud, gaming, hardware divisions affected; limited benefits
CrowdStrike	~500	5% workforce cut; AI-driven efficiency gains
Atlassian	150	Support roles trimmed due to AI automation
Consensys	~47	~7% workforce cut after acquisition
Zeen	Entire company	Startup shut down after failing to gain traction
OpenAI Hits $12 Billion in Annualized Revenue, Breaks 700 Million ChatGPT Weekly Active Users

ChatGPTの週次アクティブユーザー数は約7億人を超え、OpenAIの売り上げが、月当たり$1billion（約1400億円）を超えており、今のペースだと、2025年度の売上が$12billionに到達するという推定を紹介する記事です。

2025年のキャッシュバーン（資金消費）は$8billionに増加するとみられており、これは以前の予測よりも$1billion上方修正された数字です。この巨額な赤字は、推論・学習プロセスの両方に必要なAIデータセンターのコストによるものです。

このキャッシュのニーズに応えるため、OpenAIはさらなる資金調達を行っていますが、その中で、SoftBankによる累積出資額はすでに320億ドルにのぼっているそうです。

全てがあまりにも巨額なため、目眩がします。結果がどうなるかはわかりませんが、こんな度胸のある投資が出来るSam Altmanや孫正義さんは、歴史に残る大人物であることは確かです。

AI Chipmaker Groq Slashes Projections Soon After Sharing With Investors

AIベンチャーのGroqが、2025年度の売り上げ予想を、$2billionから$500millionへと下げたことを報じる記事です。

Groqは、当初は（NVIDIAのように）AIチップをライセンスするビジネスを目論んでいましたが、それではビジネスが立ち上がらず、自分でAIデータセンターを提供するサービス・ビジネスに切り替えました。

実績のないAIチップベンチャーにとっては、それは正しい戦略だと思いますが、売り上げを増やすためには、自らAIデータセンターを構築する必要があり、そこがネックになって成長のスピードが落ちていると解釈して良いと思います。

少し前にアナウンスした、サウジアラビアのAramcoとの$1.5billionの契約は、Groqにとっては大きな追い風ですが、その売り上げを上げるためには、データセンターの構築が必須であり、資金繰り、場所・電力の確保、スピーディな構築など、いくつかの障害をクリアする必要があります。

別の言い方をすれば、Groqは、サービス・ビジネスで勝負すると決めた結果として、単なるAIチップの会社から、AIデータセンターを構築・運営する会社に生まれ変わらなければならなくなり、そこで苦労しているのです。

How Google’s New AI Architect Plans to Spread Gemini Everywhere

Google DeepMindのCTOであり、最近、同時に、Google全体のChief AI Architectの役を兼任することになった、Koray Kavukcuogluのキャリアを紹介する記事です。「Google Geminiをあらゆるところに広める」役割を担う人物とのことです。

Kavukcuogluは、NYUでYann Lecun博士の下で博士号を取ったのち、少しの間、NECの研究所にいましたが、2012年に(Googleに買収される前の)DeepMindに入り、今に至るそうです。

経歴を見る限り、エンジニアというよりは研究者ですが、そんな人がGoogleの「AI戦略」を立てる要職につくあたりが、とてもGoogleらしいと言えます。

Googleは、DeepMindを買収する前から、大学の研究所のようなカルチャーを持つの会社でしたが、典型的な学者脳を持つDemis Hassabisが率いるDeepMindを買収し、DeepMindの重要性が会社の中で増した結果、その傾向がさらに強まっているように私には見えます。

私は、ユーザーおよび開発者の両方の立場で、GoogleとOpenAIのサービスの両方を頻繁に使っていますが、Googleのサービスは非常に使い勝手が悪く、まずはOpenAIのものを使うのが習慣になってしまっています。

例えば、生成系AIのAPIを使う場合、OpenAIのAPIであれば、既に持っているAPIキーで呼び出すだけですぐに使えますが、GoogleのAPIであれば、辿り着くのが容易ではない管理画面での設定が必要な上に、APIキーで呼び出すAPIとトークンで呼び出すAPIが混在しているというカオスな状況で、APIの呼び出しに成功するまで半日以上かかるのが普通です。

その根底には、ユーザー体験の重要性を軽視するGoogleのカルチャーがあり、これを治さない限りは、OpenAIに勝つことは難しいと思います。Googleの株の長期保有者としては、これが大きな気掛かりです。

The Electric: These Ex-Waymo Executives Are Automating Construction Equipment

元Waymoにいたエンジニアたち五人が作った、自動運転重機ベンチャーの紹介です。自動車やトラックの自動運転が可能なので、その技術をショベルカーやクレーンなどの重機（建設機械）に適用するのは当然の話です。

特にショベルカーの場合、設計通りの大きさと深さに穴を掘り・土を積み上げるのはとても手間がかかる作業なので、それを自動化出来れば、大幅にコストと時間を節約することが可能です。

しかし、実際の工作機器となるとそれはそれで別のノウハウが必要なので、私であれば既存の重機メーカーと組むビジネスを立ち上げるだろうと思います。重機側には自動操縦に必要なインターフェイスだけ用意してもらい、そこに接続するコンピュータと各種センサーを組み合わせたソリューションとして提供するだろうと思います。

そもそも、重機とコンピューターでは、耐用年数、進化のスピード、粗利率などが根本的に違うので、それを一つの製品として製造・販売するのが理にかなったビジネスだとは私には思えません。

Microsoft’s Rivals Lean on AI to Pry Away Longtime Customers

エンタープライズ系のアプリケーション（企業向けのソフトウェア）は、一旦ビジネスに導入してしまうと移行が難しいのが難点ですが、その問題を「コードが書けるAI」を活用することにより解決しようという試みが、顧客やライバル企業によって始まっていることを紹介する記事です。

移行が難しくなる一番の理由は、データベースに蓄積してしまった顧客・在庫・販売などのデータです。以前だと、データを既存のデータベースを抜き出すだけでも大変な作業だったため、それがアプリケーションの切り替えを困難にしていました。

この記事では、既存のデータベースからデータを抜き出すソフトウェアをAIに書かせることにより、移行を容易にした例が複数紹介されていますが、今後、こんな例が増えることは確実だと思います。

The Country Where 76% of Cars Sold Are Electric

ネパールで新しく販売された自動車の中での電気自動車の割合が76%になったことを報告する記事です。

ネパールの場合、大半の電力が水力発電による「自然に優しいエネルギー」であり、かつ、安価なため、メンテナンスや燃料代を考えると、電気自動車の方が安くなってしまったのが一番の原因だとこの記事は指摘しています。

この市場で活躍しているのが、安価な電気自動車を販売している中国で、この急激な「EVシフト」に乗ることが出来なかった、日本企業や米国大手は、アジア市場から締め出される形になってしまっています。

米国の電気自動車市場で圧倒的なシェアを持つTeslaも、（Model ２と呼ばれていた）安価なモデルよりも、ロボタクシービジネスを優先させることを決めたため、こういった市場に食い込むことは出来ていません。今年中にModelYから機能を削ぎ落とした廉価版を発売するとアナウンスしましたが、それでは不十分でしょう。

それもこれも、Elon Muskが「TeslaはAIで勝負する会社になる」と会社のミッションを変更した結果のものであり、仕方がないとは思いますが、その分だけ、短期的には売り上げ・利益ともに成長が鈍ると理解しておいた方が良いと思います。

質問コーナー

【質問】

中島さんが普段情報収集に使っているコンテンツを教えてください。例えば、NHK NEWS WEB・日経電子版・NewsPicksなど、具体的に教えていただけると助かります。たくさんあるのであれば、メイン使いのものだけで結構です。今後の自身のインプットの参考にさせていただきたく、可能であればオススメの理由も併せて伺えれば幸いです。

《回答》

現時点でお金を払って購読しているメディアは、BloombergとThe Informationです。それ以外の情報は、基本的にXを使って収集していますが、沼にハマらないように、読みたいと思った記事のURLだけを一旦別のところに書いておき、後から読むようにしています。

ちなみに、日経などの日本のメディアの記事で面白そうなものがXで流れてきた場合、リンクを開いても読めないことが多いので、記事のタイトルから情報ソースを検索するようにしています。

【質問】

中島さんのメルマガやyoutubeのおかげでたくさんの面白い企業を知ることができて、とても勉強になっています。さらに深掘りして自分でも企業の情報を調べたりするのですが、調べた情報を記憶しておくことが難しいため、Notionなどのアプリを使って、調べたことをまとめています。中島さんは調べた情報について、なにかノートのようなものにまとめて管理されているのでしょうか？

《回答》

私の場合、アウトプットすることが一番効率の良い勉強の方法なので、このメルマガを書くことが物事の理解を深める意味で、とても役に立っています。さらに、このメルマガが私自身にとっての備忘録の役割を果たしているとも言えます。

どうしてもちゃんとまとめておきたい場合には、githubにリポジトリを作って、そこに書くようにしています。

わかりやすい例が、「AIチップベンチャーについてまとめたページ」で、他人に見せるためではなく、自分のためのメモ帳として使っています。

【質問】

Duolingoの株を買われたという記事が気になり初めて質問させて頂きます。

私も娘（小5）もDuolingoユーザーで楽しく英語やスペイン語など楽しんでいるので、Duolingoそのものはとても素晴らしいサービスだと思っています。

一方で学ぶということを考えたときに市場はどんどん小さくなってしまうのではと最近感じています。AIがどんどん進化するなかで、学ぶ人と学ばない人が二分化され、学ぶ人はどんどんとマイノリティになり、Duolingoが提供するような学びのフィールドはターゲットが小さくなるように感じるのです。

この点、中島さんは「学ぶ」という営みの未来をどのように捉えていますか？Duolingoのような学習サービスの価値はどう進化していくとお考えでしょうか？

《回答》

AIが進化し続けると、多くの人が「学んでもAIには敵わない」と感じた結果、勉強しなくなる、という暗い未来を想像してのことだと思います。

この件に関しては、私はそれほど心配はしておらず、逆に、「AIのおかげで働く時間が少なくなったから、余った時間で勉強する」人も増えるだそうし、AIを応用した教育アプリが進化することにより、これまでコストや効率が理由で勉強できなかった・しなかった人が勉強できるようになるケースもあると考えています。

【質問】

日本株のさくらインターネットが生成AI向け大型案件終了の影響ということで営業利益を大きく落としました。

さくらインターネットに委託していた会社の生成系ＡＩがうまくいかなかったから案件終了となり、さくらインターネットの利益は落ちたのだと思いますが、海外ではそんな基調はあるのでしょうか？

正直、海外の規模に比べて日本でやっていることなんて知れてるので、世界的な生成系ＡＩの流れは全く止まらないと思うのですが、海外での雰囲気が変わってきたなどの風潮があれば教えてほしいです。

《回答》

この件に関する記事（さくらインターネット、最終利益予想を91.7％減と大幅下方修正　GPU大型案件終了が影響）を読む限り、契約の継続を期待していた一つの顧客が契約更新をしなかったため、数十億円の売り上げ減になったとのことです。

さくらインターネットは、大量のGPUをNVIDIAから購入してAIデータセンターを構築して来ましたが、このビジネスは、

莫大な資金が必要
最新のGPUを入手するのが困難
継続的に利用してくれる顧客の確保が重要
GPUの世代交代による陳腐化
などのリスクがあり、ちょっとしたタイミングで大きな損失を被る可能性があります。契約の更新をしなかった顧客がどこかは公開されていませんが、それだけの大きな顧客と複数年契約を結んでいなかったことが敗因であるように見えます。

AI業界はまだまだ黎明期で、これからますます需要が伸びることは確実ですが、ちょっとした間違いで、さくらインターネットが陥ったような状況に陥る可能性は、どのプロバイダーも持っています。

AIインフラ・ベンチャーとして上場し、注目されているCoreWeaveの株を私が購入しない理由はここにあります。上に書いたようなリスクを、公開されている情報だけから読み取り、判断するのがとても難しいからです。

【質問】

「働き方」について意見を聞かせてください。

現在のサラリーマンは一昔前(20年前くらい)に比べて、業務への投下時間が明らかに少ないと私は感じていますが、早く帰宅する分、副業で自身の能力をアップさせるのならそれもまた良い選択肢と考えていました。そのような中、副業している人の数がかなり少ないという記事を見ました。https://www.nikkei.com/article/DGKKZO89831730U5A700C2EA4000/

「量と質」の議論は以前からありますが、どんなことでも量をこなさないと成長しないと思います。仕事にも副業にも時間を投下せず、娯楽？にばかり時間投下している現代の日本人は世界で淘汰されていくだけでなく、日本人の中でも努力する者としない者との間で貧富の差が拡大していく未来が見え、どことなく虚しい気持ちがしています。ひとまずの対策として、私自身は努力を継続して仕事を楽しく頑張っていこうと思いますが、考え方含め、こうすると良いよというものがあれば教えていただきたいです。

《回答》

とても良い姿勢だと思います。何事においても「継続は力」なので、自然体で頑張れる仕事・勉強（極端は話、他人にとっては苦行が、自分にとっては楽しくて仕方がないこと）を継続的に行うことが一番だと私は思います。

【質問】

7/29のメルマガにて、報道はもっと個人にカスタマイズされたものであるべきと感じますとありました。おっしゃるとおり、パーソナライズすることで個々人に有意義な情報を提供することができるとおもいます。

しかし、パーソナライズ化されすぎてしまうと一種の偶発的な発見をする回数が減ってしまうのかと感じました。例えば、日経新聞を読んでいて自分の領域外で興味を引く記事を見つけるなどです。

将来的に報道が個人にカスタマイズされていくとすると、このような偶発性はどこで得ることができるのでしょうか。

《回答》

そこはアプリの作り方次第だと思います。TikTokが良い例ですが、気に入らないコンテンツを簡単にスキップできる仕組みになっていれば、適度な割合で「偶発的な出会い」を意図的に起こしても問題なく、かつ、「スキップするか、最後まで観るか」と言うユーザーの行動が、さらにサービスの質の向上に繋がるという好循環を生み出すのです。

【質問】

中島さんは、スーパーコンピューター（量子コンピューターを含む）の将来性について、どのようにお考えでしょうか？

近年、AI技術の進歩により、これまで解決が難しかった複雑な問題にも対応できるようになってきていると感じています。たとえば新薬の開発などでは、従来はスーパーコンピューターを用いた膨大なシミュレーションや、ある種の総当たり的な手法が使われてきたかと思います。しかし、最近のAIは、大量のデータからパターンを学習し、より効率的なアプローチで結果を導き出すことが可能になっており、必ずしもスーパーコンピューターのような brute-force 型の処理に頼る必要がなくなってきているのではないかと感じています。このままいくと、AIで事足りる世界になっていくのか、AIが得意な領域とスーパーコンピューターが得意な領域で棲み分けされていくのか、はたまた、AIがスーパーコンピューターを使う世界となるのか、中島さんはどのようにお考えでしょうか？

《回答》

ニューラルネットを活用したソフトウェア（Software 2.0）が様々な問題を解決するようになった今、従来型の（人間が作ったアルゴリズムを実行する）スーパー・コンピュータの使い方は減っていくと見ています。たとえ計算の絶対量が減らないとしても、ニューラルネットを使った計算量が莫大になるため、比率で言えば統計データにも載らないような小さな割合（0.1%以下）になってしまうと思います。

【質問】

いつもメルマガを楽しく、また貴重な学びの場として拝読しております。本日はご質問をさせてください。Accounting やFinance の仕事をしていますが、現在の会社がAIの発達によりCorp HCの削減方針を出しています。その中で自身の業務負荷を減らし、よりたくさんの仕事が出来るようにしたり、自身のスキル向上を目的に、これまで抵抗のあったPythonをAIやYouTubeを使って勉強して行こうと考えていますが、何か注意点やアドバイスをいただけますと幸いです。

《回答》

Pythonは、Finance系の仕事ととても相性が良いので、目の付け所はとても良いと思います。AIによりコーディングが盛んになる前から、データ・サイエンティストと呼ばれる人たちは、Jupytor Notebook（インタラクティブなPythonの実行環境）を使って、使い捨てのコードを書いてデータ解析を行って来ました。

理想的には、コードが書けるLLMとJupytor Notebookの組み合わせが最強だと思いますが、実際に構築すべき開発環境については、私自身が使ってもいないものを勧めることもできないので、ネットで調べていただけると良いと思います。

【質問】

仕事で必要な行政手続きを調べる際に、AIに関連する法令や条例、規則等について尋ねることが多いのですが、回答の内容は概ね合っていても、該当する箇所（第～条～項など）が間違っていることが多いです。

以前、AIが苦手ななぞなぞの話でおっしゃっていた「規模言語モデルは、単語に相当する「トークン」単位で文字列を処理しているため、一文字づつ処理するのは得意ではありません。」というのが、この箇所を見つけられない理由なのでしょうか。

《回答》

大規模言語モデルは、学習プロセスで与えられたデータを「記憶」していますが、そのまま記憶しているわけではなく、ニューラルネットのパラメータとして圧縮した形で記憶しているため、その記憶に基づいた文章が必ずしも正確ではないという欠点があります。

特に、日本の法令・条例などは、ネット上に存在するコピーの数も少ないし、米国で大規模言語モデルを作っている人たちにとっても重要なデータではないため、他のデータ（例えば米国政府の法令）と比べると記憶の精度も低くなります。

【質問】

ファインチューニングで観光AIチャットボットをPythonで開発・デプロイしていますが、現状も期待通りに応答されず、情報を更新しても改善されません。RAGには別の課題があり、今回はファインチューニングで完成させたいと考えています。現在は外注も検討しており、クラウドファンディングのような形で運用資金も集めています。完成度を高めるにはどうすればよいか、開発費の目安とあわせてアドバイス下さい。

《回答》

RAGが持つ「別の問題」が何なのかが不明ですが、ファインチューニングは手間と時間がかかるし、データの更新には弱いし、LLMを切り替えることも容易ではないので、この手のアプリケーションには不向きだと私は思います。私が作るのであれば、RAGで実装します。

私は受託での開発はしないので、見積もりは難しいですが、私が作るのであれば、まずは実際のデータを元にした簡単な（RAGの）プロトタイプを一週間ぐらいで作り、まずはその性能を見ながら（＝顧客のフィードバックをもらいながら）、徐々に改良していくようなアプローチを取るだろうと思います。
