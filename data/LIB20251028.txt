週刊Life is beautiful ２０２５年１０月２８日号：Digital Euro、脳の不思議、Codex やClaude Codeの使いこなし方

今週のざっくばらん

Digital Euro

「デジタル・ユーロ」に関する記事（ECBのデジタル通貨、1人50万円に制限案　預金流出抑え銀行安定）を目にしたので、少し調べてみました。

欧州中央銀行が「Digital Euro」というページを公開しているので、それがとても良い参考になります。

まず最初に、最も重要な点を指摘しておくと、「デジタル・ユーロ」は、欧州中央銀行が発行する「デジタル通貨」であり、カテゴリーとしては、CBDC（central bank digital currency）と呼ばれるものです。

CBDCは、国の中央銀行が発行する「法定通貨」であり、「元」や「ユーロ」などの紙の法定通貨と全く同じ価値を持つものです。CBDCは、法定通貨立てのデジタル通貨という意味では、今話題の「ステーブル・コイン」ととても良く似ていますが、本質的には大きく異なるものです。

しかし、類似点の多さゆえに勘違いしている人も多いし、その誤解を利用して儲けようとする悪い人たちもいっぱいいるので、注意が必要です。

以下に違いを箇条書きにします。

発行者：CBDCの発行者は国の中央銀行ですが、ステーブル・コインを発行するのは国とは独立した法人（ほとんどの場合は営利企業）です。
管理者：CBDCの管理は中央銀行が行いますが、ステーブル・コインは、ブロックチェーン上のソフトウェア（スマートコントラクト）により自動化されたもので、非中央集権的に設計されています。しかし、非中央集権的とは言いながらも、発行者が多くの権力を持ち続けることが多いので注意が必要です。
法定通貨との関係：CBDCと法定通貨との交換は中央銀行により保証されています。ステーブル・コインは、それを発行する会社が、発行額と同等の法定通貨や国債を持つことにより、法定通貨との交換を保証しています。
リスク：CBDCは法定通貨を発行している中央銀行自身が発行しているため、法定通貨との交換に関してのリスクはゼロです。ステーブル・コインの場合、それを運営する会社が約束通りに発行額と同額の法定通貨や国債を保有していれば問題はありませんが、その約束が破られてしまうというリスクがあります（実際に、それによって破綻してしまったステーブル・コインが複数あります）。
テクノロジー：ステーブル・コインはパブリック・ブロックチェーンをテクノロジーとして使いますが、CBDCの場合は、（１）通常のデータベースを使う、（２）パブリック・ブロックチェーンを使う、（３）プライベート・ブロックチェーンを使う、などのいくつかの選択肢があり、まだどれが主流になるかは決まっていません。
欧州中央銀行は、CBDCをどのように実装するかを現在も検討中です。ブロックチェーンを使う場合、特にパブリック型では次のような課題が指摘されています。

マイニングによる膨大な電力消費
トランザクション速度の遅さと高コスト
技術的に未成熟で、電力効率と安全性の両立が難しい
こうした理由から、通常のデータベースで実装すべきだと考える専門家は多く、私もその一人です。ブロックチェーンは間違いなく革新的な発明ですが、その真価が最も発揮されているのは、中央銀行などの権威に依存しないビットコインのような非中央集権型通貨においてです。

一方で、国家が発行する法定通貨をデジタル化するのであれば、信頼性・効率・ガバナンスの観点から、CBDCを中央集権的に運用するほうが合理的だと私は考えます。

米国では、（トランプ氏を再び支える人々の影響もあり）ステーブルコインを国家として支援する方向に動いています。しかし、これは大きな誤りでしょう。通貨をデジタル化するなら、公的なCBDCを、通常のデータベース技術で実装するのが筋だと思います。

脳の不思議

尊敬する、Andrej Karpathyのインタビュー（参照：Andrej Karpathy ― “We’re summoning ghosts, not building animals”）を観ていて、色々と学んだこと・刺激を受けたことがあるのですが、「脳の不思議」という観点からいくつか重要なことを書いてみます。

まず第一に、人間に限らず、動物は既にかなりのことを学んだ状況で生まれて来ます。分かりやすい例は、牛の赤ん坊です。彼らは生まれてすぐに立ち上がり、あっという間に走り始めてしまいますが、これは既に脳の中に、どうやって走るべきかの情報が刻まれていることを示しています。

これは（AIモデルの）ニューラルネットで言えば、各ノードのウェイトが既に適切な値に初期化された状態で生まれてくることを示しており、学習をさせなければ何もできないニューラルネットとは大きく違います。

私たちは、地球上の生物が、時々起こる遺伝子の突然変異と、自然淘汰の仕組みで進化して来たことを知っていますが、それだけでは、親が学んだことがどうやって子供に伝えられるのかが説明できません。DNAが脳内のニューロンを繋ぐシナプスの初期化に関わっているから、初期値に関しても突然変異と自然淘汰の結果の進化が起こったと説明できないことはありませんが、それでは進化のスピードが遅すぎるように直感的には感じられます。脳の進化に関しては、私たちがまだ発見していない、別の仕組みがあるのではないかとワクワクしてしまいます。

強化学習に関しても、Andrejはとても重要なことを指摘しています。強化学習とは、正しい答えが明確な場合に、ニューラルネットに同じ問題に対する推論を何百回も何千回も行わせ（ここで大量の電力を消費します）、正しい答えに辿り着いた道筋を遡って、それらのノードの重みを変更する手法です。

単純なニューラルネットであればそれで十分ですが、今のフロンティア・モデル（最先端のモデル）のように深く考えるニューラルネットの場合、必ずしもまっしぐらに正しい答えに辿り着いているわけはなく、その過程では、間違った道筋を辿ったり、遠回りをしている可能性も十分にあり、それらのノードの重みを一律に変えてしまうことには無駄が多いと彼は指摘するのです。

人間が学習する際には、たとえ正しい答えに辿り着いたとしても、その考えのプロセスを遡って、その中で正しかった考え方と間違っていた考え方をしっかりと見極めた上で、正しかったプロセスのみを学ぶようにします。

結果として、人間はニューラルネットと比べて、はるかに効率良く物事を学べるため、エネルギー効率がとても良いし、推論しながら学び続けることすら可能です。

OpenAIがChatGPTにメモリー機能を導入して以来、過去に話したことを少しは覚えてくれるようになりましたが、人間の脳のように学習しているわけではなく、過去に話した事柄を書いたメモを見ながら推論をしているだけの話です。なので、人間のように、一度解くことができた数学の問題を次の日に解ける保証は全くないのです。

CodexやClaude Codeの使いこなし方

先日、OpenAIのエンジニアたちがCodexをどう活用しているかを紹介しているビデオを見ました（リンクを紛失してしまいましたが）。

いくつか参考になるテクニックが紹介されていましたが、もっとも役に立ったのは、プランニング・ドキュメントの作成です。

CodexやClaude Codeにコードを書かせていると、途中からあらぬ方向に進んでしまい、その収拾に手間がかかることがあります。また、コンテキストが長くなると、最初の方に話したことを忘れてしまうことがあるし、一度、コンテキストを切ってしまうと、初めから説明しなければならなくなってしまいます。

そこで活躍するのが、プランニング・ドキュメントです。

まず最初に、どんなアーキテクチャでどんな変更をしたいかをClaude Code（もしくはCodex）に説明した上で、すぐにはコードを書かせずに、"PLAN.md"のようなプランニング・ドキュメントを書かせます。そしてそのプランニング・ドキュメントを丁寧にレビューした上で、指示した通りのアーキテクチャになっているか、なぜそんなアーキテクチャにしたのか、などを確認し、問題があれば修正させます。

納得できるプランニング・ドキュメントを書かせた上で、それに従ってコーディングをさせます。複数のステップに渡る大きな変更の場合は、進捗に応じて、どこまで進んだかをプランニング・ドキュメントに反映させます。

実際にこの手法でコードを書かせると、かなりの高い確率で期待していた通りのコードを書いてくれるし、コンテキストが長くなってしまった場合には、一旦止めて、リスタートした上でプランニング・ドキュメントを読んだ上で、続行するように指示することも可能です。

MulmoChatの開発においても、この手法を取り入れたところ、とても効果的でした。結果として、作られた、PLAN.md、PLAN2.md、PLAN3.md というファイルがリポジトリに残っているので、興味のある方は参照してください。

特に、PLAN4.mdは、Googleのnano bananaに加えて、ローカルで動くComfyUIからも画像生成ができるようにした際に、特定のtool pluginのためのUIがconfigページに混ざってしまったことを嫌ったリファクタリングで、結構面倒なコード変更が必要でしたが、Claudeが、「リファクタリングをする理由」をしっかりと理解してくれた結果、とても良い仕事をしてくれました。

PLAN4.mdを見ていただければ分かりますが、冒頭に「なぜこの変更が必要か」が丁寧に書かれている上に、データ構造への変更から実際のコードもかなり含まれているため、この段階で重要な部分のコードレビューは終わっており、実際のコードの変更は、すべてそのままの形で受け入れる形で素早く進めることが出来ました。

AIを使ったコーディングに関しては、「それほど生産効率は上がっていない」「逆に手間が増えている」などの批判がありますが（参照：Busting the AI coding productivity myth、Research: quantifying GitHub Copilot’s impact on developer productivity and happiness）、それは開発者たちが使い方に慣れていないからだと私は思います。

私の目に留まった記事

Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers

著作権で保護された書籍をAIモデルの訓練に使用することは、AIが派生的な作品を生成できるのではないかという懸念から、多くの作家による訴訟を引き起こして来ました。しかし、これらのモデルが本当に作家の文体を模倣しつつ、高品質な文学的文章を生成できるのかは不明である。

この論文は、そんな疑問に答えるために行われた研究の紹介です。

研究者たちは、まずは、ChatGPT、Claude、Geminiなどの一般的な大規模言語モデルに、著名な作家たちのスタイルを真似た文章を書かせ、一般の人たちと、文学の修士号を持っている人たちの二つのグループに読ませて評価させたそうです。

すると、一般の人たちの評価はさまざまでしたが、専門家たちの評価は低く、「AIが書いた文章」であることは明らかだったそうです。

しかし、ChatGPTを個々の作家の全作品でファイン・チューニングしたところ、大きく結果が変わり、一般の人たちだけでなく、専門家たちも生成された文章を高く評価し、AI独特の言い回しなどもほぼなくなったそうです。

つまり、冒頭に書いた「AIが特定の作家の派生的な作品」を作ることは十分に可能だということです。

ちなみに、著作権で保護された作品をAIの学習過程で使うことは、人間には許されているのにAIには許されないというのはおかしい、というのがAIの開発者たちの主張です。しかし同時に、AIによる学習は「情報の圧縮」でしかないことはAI研究者の間では常識であり、単に圧縮したデータを保存して再生するのであれば、著作権を侵害していることになります。これは、人間が記憶に頼って、他人の著作物を再現することと同等です。

私は、AIによる著作物の学習を止めることは現実的ではなく、注目すべきはそのアウトプットで判断するしかないと思います。他人の著作物で勉強した人間が作る作品と同様に、その作品の類似性などから著作権違反かどうかを判断するのが現実的です。そして、特定の作家の作風を模倣した派生作品に関しては、これも人間が作る作品と同様に、既存の著作物をそのままコピーしているのでない限り、認めるべきだと思います。

つまり、「となりのトトロ」や「風の谷のナウシカ」をそのまま再現した作品は著作権で取り締まるべきですが、「ジブリ風」の絵でオリジナルを作ることは構わないのです。

例えば下の画像は、浦島太郎の主人公が女の子だった場合の１シーンをジブリ風にnano banana（Googleの画像生成AI）に描いてもらったものです。こんな作品を著作権違反で取り締まるのは間違っていると私は思います。



DeepSeek-OCR: Context Optical Compression

先週読んだ論文の一つです（Claudeの要約では納得できなかったので、本文も読みました）。DeepSeek独自のOCR（文字読み取り）モデルに関する論文です。

興味深いのは、初期の段階で文字を含んだ画像を画像トークンに変換する部分です。通常のLLMは、文字列をトークンに変換して処理しますが、Unicodeから構成される単語を分割するため、とても冗長だし、日本語のように１文字を１バイトで表せない文字は不利だし、絵文字も絵として認識できないなどの欠点があります。

それと比べると、(PDFなどに)印字された文字列を画像と処理すれば、はるかにトークン数が少なくなるし、日本語や絵文字も平等に処理できるし、さらにボールドフォント（太字）などによって強調された情報も失わずにトークン化することが可能になります。

文字を画像としてトークン化する手法は、論文中のFig.3に記述されていますが、まずは１ページの画像を16x16ドットの画像に分割します（元の画像が1024x1024だとすると、4096個の画像）。それを、画像向けのTokenizerを使って1/16に圧縮し（4096個 → 256個）、それらのトークンのEmbedding Vectorを作ります。



仮に１ページに1000個の単語が書いてあったとすると、文字列向けのTokenizerを使うと、二つ以上のトークンになってしまう単語があるため、トークンの数は2000個近くなってしまいます（約８倍）。この論文の「文字列は画像としてトークン化した方がトークン数が少なくて済む」という主張のエッセンスはここにあります。

LLMが処理をする複雑度は、コンテキスト中のトークンの数の二乗に比例するため、コンテキストが長くなると計算量が増えてしまいますが、画像トークンを使えば、この複雑度を大幅に抑えることが可能になります。

さらに興味深いのは、コンテキストが長くなりすぎた際に必要となる圧縮も、テキストであれば文字数を減らすという形での圧縮しか出来ませんが、画像として圧縮すれば、より曖昧な形での圧縮が可能になると期待できます。

この話は、単にコンキクストの圧縮だけではなく、LLMに記憶力をつける話とも関連するので非常に興味深いと感じました。過去の会話を画像トークンとして記憶させておけば、古い記憶から圧縮していくことにより「徐々に記憶を薄れさせる」ことが可能になります。

ちなみに、この論文で発表されたのは、LLMではなくOCRモデルですが、今後、画像トークンを使ったLLMの開発が活発にされることが期待できます。最近、nanochatというオープンで小型のLLMを発表したばかりのAndrej Karpathyは、「画像トークンを使ったnanochatを作ってみたいという気持ちと戦う必要がある」と発言していて笑えました（参照：I quite like the new DeepSeek-OCR paper.）。

【追記】AndrejにElon Muskが「長期的には、AIモデルの99%の入出力は光子になる。それ以外の方法ではスケールしない。(Long-term, >99% of input and output for AI models will be photons. Nothing else scales.)[https://x.com/elonmusk/status/1980430707706196359]」とリプライを返しているのを発見しました。

Autonomy, the future of auto

電気自動車、そして自動運転という二つの大きな変化についてのとても分かりやすい文章です（下にChatGPTによる要約を載せておきます）。日本には、いまだに「まだまだハイブリッドの時代が続く」「トヨタが本気を出せば、テスラに負けるはずがない」と主張する人が多くいますが、もっと根本的・本質的な変化が自動車業界には起こっていることを理解すれば、これがいかに危機的な状況なのかが理解できると思います。

自動車産業はいま、100年前の内燃機関と組立ライン以来となる大変革の入り口に立っています。それは「自動運転（Autonomy）」という革命です。

EVの登場も大きな変化でしたが、自動運転はその比ではありません。これは単なる駆動方式の進化ではなく、AI・センサー・リアルタイム演算・ソフトとハードの完全統合によって、車が「ロボット」へと進化することを意味します。

自動運転を実現できるのは、電動パワートレインとソフトウェア定義型車両という新しい技術基盤を持つ企業だけです。

従来の自動車に自動運転を後付けするのは「ガラケーにアプリストアを載せるようなもの」で、技術的にも経済的にも無理があります。その結果、米・欧・日・韓のレガシー自動車メーカーはすでに取り残されています。

真のイノベーションを起こしているのは中国勢とテスラだけです。テスラは、実世界データで学習したビジョンベースの自動運転AIを持ち、車・ロボット・機械全体を統一的に制御するAIアーキテクチャを完成させました。

この領域でテスラに並ぶ企業は存在しません。これからの時代、自動車産業を支配するのはテスラと中国メーカーです。ガラケー時代の巨人たちは、スマートフォンの時代に取り残されるでしょう。

テスラ＝自動運転。その歴史的瞬間が、いま始まろうとしています。

$AMD $AMZN partnership

OpenAI、Nvidia、Oracleの三社の間での巨大な取引の陰で、AmazonとAMDの間でかなり重要な取引が進んでいることを指摘するXへの投稿です。

この記事によると、AMDのCEO、Su博士は、今後、毎年$10～$20B（約1,500～3,000億円）のEPYC（AIサーバー向けのCPU）の売り上げをAmazonから期待していると発言したそうです。

AmazonがEPYCをEC2インスタンスのCPUとして選ぶ一番の理由は、コストと電力で、AMDとの共同開発の結果、電力あたりの計算能力を50％向上させ、設備投資に必要なお金を年間$20～$30B節約し、結果として、粗利を30～40%向上させることになるとのことです。

AIの学習プロセスにおけるNVIDIAの独占状態には揺るぎはないものの、推論市場が拡大するにつれ、Googleは独自のTPUに、AmazonはAMDとのパートナーシップに力を入れ始めている点は注目に値します。

ここのところGPUばかりが注目されていますが、推論を大規模なサービスとして提供する際にはCPUも重要であり、CPUとGPUの両方を持つAMDとしては、CPUの優位性・重要性を高めることが戦略上重要であり、このAmazonとの提携は、その戦略の成果と言えます。

文章中に、「推論の遅延にはCPUが重要な働きを持つ」と書かれていますが、これはとても重要です。GPUは、tokens/secのような計算スピードに大きな役割を果たしますが、GPUに必要なデータを渡す準備をするCPUが遅延を決めるのです。つまり、どんなに計算量が高いGPUを持っていても、CPUの能力がそれに追いついていなければ、せっかくのGPUの計算能力を100%活かすことが出来ないし、生成が終わるまでの時間も長くなってしまいます。

Alibaba Cloud claims to slash Nvidia GPU use by 82% with new pooling system

一つ上の記事とも関連する話ですが、中国のアリババが、Aegaeonと名付けられた「GPU プーリング」システムを使うことにより、GPUへの負荷を82%減らすことに成功したことを報告する記事です。

アリババの研究者が韓国ソウルの学会で発表した論文によれば、さまざまなAIモデルの推論プロセスにおいて、必要なNVIDIA H20の数を、1,192個から213個に減らすことに成功したそうです。

複数の異なるAIモデルをサポートする場合には、不要なメモリ転送を避けるために一つのGPUには一つのAIモデルしかロードしないのが一般的ですが、その方法だとGPUの稼働率が低くなってしまうし、特定のモデルのニーズが急に増えた場合、必要なGPUが空くまでの待ち時間が長くなってしまいます。

Alibabaの研究者は、１つのGPUで複数のモデルをサポートすることを可能にしつつ、モデルの切り替えスピードを97%減らすことにより、GPUの稼働率が高く、かつ、待ち時間も少ないシステムを構築することに成功したそうです。

Deep Dive into LLMs like ChatGPT

Andrej Karpathyによる「ChatGPTのような大規模言語モデルは、どうやって動いているのか」を解説するYouTubeビデオです。3時間半という長尺なビデオですが、ものすごく分かりやすい、素晴らしいビデオです。

大規模言語モデルの作り方から、動く原理、活用の仕方などを丁寧に説明しているので、これから勉強する人に最適ですが、すでにある程度勉強した人たちにとっても、自分が理解していることの確認のためにも良い「おさらい」になると思います。

AI Accelerator Market Revenue per GW - Growth Potential of AVGO, NVDA, and AMD (JPM)

JP Morganによる、GPU市場の予想ですが、Broadcom、NVIDIA、AMDそれぞれの1GWあたりの売り上げ予想になっている点が興味深いです（AIデータセンターの規模を電力規模で示すのがスタンダードになりました）。

私は、3社のいずれの株を持っており、NVIDIAの株が他よりも一桁多いのですが、リバランスをするべきかどうか悩んでいます。Broadcomは、GoogleのTPUに代表されるカスタムASIC（XPU）が大きく伸びているし、AMDも着実にAIデータセンター向けの売り上げを伸ばしています。とはいえ、学習プロセスにおけるNVIDIAの強さは相変わらず圧倒的です。市場全体が成長しつつ、後発だったBroadcomとAMDがシェアを伸ばすという展開になるように私には思えます。1GWあたりの売り上げにはもっと大きな差がある（NVIDIAが他を大きく引き離す）のではと、直感的には思います。

以下はGPTによる日本語訳です。

1. 市場全体の見通し

大規模AIコンピュート契約の拡大により、NVIDIA、AMD、BroadcomのAI収益ポテンシャルに注目が集まっている。
JPモルガンは、データセンターの収益を1GW（ギガワット）あたりで試算し、今後3～4年のAI関連収益の上振れ余地を分析。
AIアクセラレータ市場は年率40～50%成長が見込まれ、GPU中心からXPU（カスタムAI SoC）への多様化が進行中。
資金調達・競争・実行リスクは残るが、OpenAIやGoogleなどの大型契約による上方修正の可能性が高い。
2. Broadcom（AVGO）

OAIとの10GW規模のカスタムAIチップ＋ラック供給契約がAI収益構造を大きく変える。
XPUシステムはGPUより価格30%安く、効率30%高い。結果、1GWあたり約270億ドルの収益。
GoogleのTPU v6p（3nm）参考値とも整合し、ネットワーク＋ラック構成比を含め1GWあたり約280億ドルと推定。
10GWが3～4年で展開されると、2027～2029年に年商700～900億ドル規模となる可能性。
市場予想（600億ドル）より50～70%上振れ余地。GoogleのTPU収益（約200億ドル）を含めるとAI収益は1,000億ドル規模に達する見込み。
3. NVIDIA（NVDA）

経営陣は1GWあたり350～400億ドルの収益機会があると発言（データセンター設備投資の約60%）。
Blackwell（BW）/Ultra（BWU）は27～29B$/GW、Rubin（R）/Ultra（RU）は30～35B$/GWと推定。
ラック電力密度は120kW → 1.1MW（約9倍）に上昇。GPU数は減るが、GPU単価は約3倍に。
GPU単価は約5.2万ドル（BW）→ 約30万ドル（RU）へ上昇見込み。
2027年までに10GW規模のAIデータセンター展開が想定されるが、実際はそれを上回る可能性あり。
今後2～3年のAI収益予想は上方修正が見込まれる。
4. AMD

OpenAIとの契約は数十億ドル規模で、少なくとも1GWあたり200億ドルと推定。
HeliosプラットフォームはNVIDIA Rubinより20%安価で、ネットワーク収益が全体の約5%。
MI450（Helios）は20B$/GW、次世代MI500は25B$/GW超と見込む。
OpenAIの6GW計画が実現すれば、年商300～350億ドルの可能性。
現行市場予測（2027年データセンターGPU収益310億ドル）より上振れ余地あり。
5. 比較まとめ

企業	1GWあたりのAI収益推定	特徴
Broadcom (AVGO)	約280億ドル	ネットワーク・ラック含む広範な構成
NVIDIA (NVDA)	約350億ドル（Rubin Ultra）	GPU単価上昇で高収益化
AMD	約200億ドル（Helios）	価格競争と大口顧客獲得でシェア拡大
6. 投資見通し

2026～2028年にかけてAI需要加速で大幅な増益余地。

各社の強み：

NVIDIA：高性能プラットフォームでリーダー。
Broadcom：XPU＋ネットワーク統合で優位。
AMD：Helios中心の大型顧客展開でシェア拡大。
AIインフラ投資が継続する限り、3社とも再評価（re-rating）・バリュエーション拡大が見込まれる。

OpenAI Has a Business Plan

Sam Altmanが6年前に「OpenAIはどうやってお金を稼ぐのか」という司会者の質問に「現時点ではどうやって稼ぐかは知らないけど、超知能を作っているので、それが完成したら、それに相談すれば良いと思う」と答えた話（Sam Altman in conversation with StrictlyVC）を紹介した上で、現状のOpenAIについて、アップデートしている興味深い記事です。

今では、ChatGPT PlusやChatGPT Proでしっかりとした売り上げを上げる会社にはなっていますが、いまだに莫大な赤字を垂れ流しており、Sam Altmanの言う超知性を作るには$2T(約300兆円)が必要とされています。この「AIブーム」に乗り遅れまいと、大量のリスクマネーが集まっているとは言え、流石にその規模になると、投資だけでなく、融資を受ける必要がありますが、融資向けのお金＝確実なリターンであることを考えると、そこにミスマッチがあると、この筆者は指摘します。

ちなみに、ちょうどこの記事について書いている時に目に入ってきたのが「Nvidia Discusses Loan Guarantee for OpenAI」という記事です。

NVIDIAは既に、OpenAIに対して$100B（約15兆円）を投資して10GW規模のAIデータセンターの構築をサポートすることを発表しましたが、それだけでは不十分なOpenAIは、（上の記事に書いてある通りに）融資で更なるお金を集めようとしていますが、その融資に対して、NVIDIAが「$350B（約52兆円）の債務保証」を提供する話が進んでいる、というリーク情報があるそうです（How Sam Altman Tied Tech’s Biggest Players to OpenAI）。

OpenAIに対する$100Bの時も、「循環取引ではないか」と批判する人もいましたが（正確には循環取引ではなく、ベンダーファイナンスです）、債務保証となればもっと批判する人が増えることは容易に想像できます。

Sam Altmanの作り出す「現実歪曲空間」は、とうとう国家予算の規模のお金を動かすまでになったことは、単にテクノロジーの話ではなく、歴史的に特筆すべき大きな動きだと言って良いと思います。

【追記】この件の背景には、OpenAIがGoogleのTPUを推論に使うという話があった、という説が流れています。NVIDIAはその流れを止めるために、OpenAIに対して、巨額の投資や債務保証をせざるを得なかったのだ、という説です。どこまで本当かは分かりませんが、あっても不思議はない話です。

New autonomous strike drone completes trials with 1,000-mile range, 88-lb payload

ロシアとウクライナの間の戦争が、ドローンを活用した近代戦闘にシフトしつつありますが、電波妨害されてしまうリモコン操縦では不十分なため、長いワイヤを繋げたまま操縦するドローンに続いて、GPSを活用した自動運転型のロボットが活用されるようになりましたが、それも、GPSの電波をジャムする技術を双方が使うようになったため、更なる進化圧がかかっているそうです。

この記事が紹介しているのは、GPSに頼らず、カメラで捉えた映像をドローン上AIでリアルタイムで認識しつつ、ターゲットに向かうドローンです。

広島・長崎に落とされた原子爆弾は、第二次世界大戦という進化圧が作り出した兵器ですが、ロシア・ウクライナ戦争の進化圧が、ドローン技術の発展を促していることには注目しておくべきです。一時期、中国のDJIが圧倒的な強さでドローン市場を席巻してしまいましたが、これをきっかけに米国政府も目を覚まし、Andurilのような新手の軍事企業を育てることに繋がっています。

Continual Learning via Sparse Memory Finetuning

これは、UC Berkeley と Meta の研究者による論文で、これまで不可能だったLLMの継続学習（学習済みのLLMに新たな知識を加えること）を可能にする画期的な手法、「スパースメモリファインチューニング」が提案されています。

ChatGPTやClaudeなど、現在広く使われている大規模言語モデル（LLM）は、すべてGoogleの研究者が提案した「トランスフォーマー」アーキテクチャを基盤としています。このトランスフォーマーは、主に以下の2つの部分から成り立っています：

セルフアテンション層：入力トークン間の関係性を学習する仕組み
フィードフォワードネットワーク（FFN）層：各トークンごとの非線形変換を行う層
ところが、すでに学習済みのLLMに対して新しい知識を覚えさせようとすると、過去に覚えたことを忘れてしまう「破滅的忘却（Catastrophic Forgetting）」が起こることが知られています。この「破滅的忘却」が、これまでLLMの継続学習を困難にしてきた大きな課題なのです。

この論文の研究者たちは、「破滅的忘却」の主な原因が、トランスフォーマーの中のフィードフォワードネットワーク（FFN）層にあることに着目しました。FFN層は巨大なパラメータが密に結合した層で、新しい知識を覚えさせる際に全パラメータを変更しなければならないため、過去の知識が上書きされやすいのです。

そこで研究者たちは、従来のFFN層を「メモリレイヤー」と呼ばれる疎結合で動的参照可能な層に置き換えることを提案しました。

メモリレイヤーは、外部に配置された巨大なメモリテーブル（ルックアップテーブル）のような構造を持つ層です。各入力に対して、メモリ全体の中から関連性の高いごく一部の「スロット」だけを動的に参照し、必要に応じて更新できます。この置き換えによって、モデルは全パラメータではなく、タスクごとに必要な一部のメモリスロットだけを更新する形になり、新しい知識を追加しても過去の知識を壊しにくくなる、という仕組みが生まれます。

もともとメモリレイヤーは、LLMの外部に置かれた知識を推論の際に参照するために提案された技術でした。しかし、この論文では、メモリレイヤーに格納される知識を外部から静的に渡すのではなく、学習の過程で動的に生成・更新していくように改良しています。これにより、モデルは新しいタスクを学習しながら、必要なメモリスロットを自ら作り出し、過去の知識を保持しつつ新しい知識を加えることができるのです。

継続学習は、LLMを人間の脳に近づけるための重要なステップなので、この研究は、今後のLLMの開発に少なからず影響を与えるだろうことは期待できます。

Record $38 Billion Debt Sale Nears for Oracle-Tied Data Centers

Oracleが、AIデータセンターの構築費として、$38B（約5,700億円）を融資（社債）の形で調達することを報告する記事です。

Oracleは、OpenAI向けのStargateプロジェクトを含め、トータルで$500B規模のAIデータセンター投資をすると宣言していますが、これはそれに必要な資金調達の一部です。テキサス州のデータセンターに$23.25B、ウィスコンシン州のデータセンターに$14.75Bを投じるとのことです。社債の引き受けとしては、JP Morgan、三菱UFJ、Wells Fargo、BNP Paribas、Goldman Sachs、三井住友、などの名前が上がっていますが、ソフトバンクの名前はありません。

期間は4年間で、プラス2年まで延長が可能、金利は基準金利＋2.5%だそうです。基準金利は米国債利回りによって決まりますが、現時点では4.0～4.5%なので（期間によって異なる）、6.5～7%になります。「基準金利＋2.5%」とは社債の格付けだとBBのハイイールド債（ジャンクボンド）領域に入り、投資家からは、かなり高リスクな融資と見られていることを示します。

Stargateプロジェクトのアナウンスは、OpenAI、ソフトバンク、Oracleによりホワイトハウスで行われましたが、その後のアナウンスにおいては、ソフトバンクが外されることが多く、側から見ると、ソフトバンクとOracleの関係は必ずしも良好ではないようです。表面的には、ソフトバンクとOpenAIの間のものだったStargateプロジェクトに、Oracleがトランプ大統領との関係を利用して割り込み、自分のビジネス（AIデータセンターの構築と運営）にしてしまい、単にお金だけを出す役割に陥ることをソフトバンクが嫌がっているように見えます。

少し前にも書きましたが、私はAIが社会を大きく変えることについては確信を持っており、中長期的にはとても強気ですが、一時的にバブル崩壊のようなショックが来る可能性は十分にあると見ています。その際に、一番厳しい状況に晒されるのは、自社のキャッシュフローで支えられる以上の借金をしてインフラ投資をしている会社なので、注意が必要です。

質問コーナー

【質問】

シリコンバレー・テック企業で導入されている Forward Deployed Engineer (FDE ) という職種に関してお伺いさせてください。私も少し株を保有していますが、Palantir Technologies が先駆けとなり職種を導入する企業が急増している・・・とのこと。

FDEの働き方としては

顧客のオフィスや現場に直接出向く
課題を自分の目で見て、その場で解決策を実装
のような、働き方だけを見るとSESや客先常駐のエンジニアのように見えますがより高度なスキルが求められているのかなと感じます。（ある種Palantir の価値の一つなのでしょうか）

質問としては

生成AIネイティブなこの時代において、生成AIシステムの実装なども含めたこのFDEという職種がスタンダードになり、今後より拡大していくことは、トップエンジニア企業でも考えられるでしょうか。ソリューションアーキテクトと似たポジションとも思えました。
日本の企業においてもこの職種が広がっていくことは考えられるでしょうか。既に顧客側に常駐するコンサルタントやエンジニアの役割がある中で、ポジションが定着するでしょうか。
《回答》

これは、「Palantirのビジネスが、なぜこれほどまでに急成長しているか？」だけでなく、私が批判してきた日本のSIerのゼネコンスタイルとも深く関係する話です。

日本の大手SIer（ITゼネコン）の場合、顧客と直接やり取りをするのは、コードが書けない・書かないセールス側の人たちで、彼らの仕事は、顧客向けのプレゼン資料を作ったり、仕様書を作ったりする、いわゆる「上流」の仕事です。彼らが仕事を受注した後は、その仕様書に基づいて、詳細設計が行われ、それがさらに下流のエンジニアたちに流れて、実装されます。つまり「ニーズの把握・設計・実装」が分断されているのです（さらに悪いのは、実装の部分が子会社や孫会社に丸投げされ、かつ、そこには理系の大学でコンピュータ・サイエンスを勉強していた人たちがいない、という現状があります）。

大勢の人々が関わる分、効率も悪いし、良い設計ができませんが、ビジネスモデルは「人月工数で稼ぐこと」なので、それで構わないのです。「たくさんの労力をかけて仕事をする」ことがより多くの売り上げに繋がるビジネスモデルです。

一方のPalantirは、理系の大学どころか、修士号・博士号を持つトップクラスのソフトウェア・エンジニアが顧客のところに常駐し、顧客に実際の価値（売上増、業務の効率化、在庫の削減など）を提供するものを直接システムに組み込む形で作り上げた上で、（人月工数ではなく）その価値に対する対価をもらうビジネスモデルです。

Palantirのエンジニアの仕事は、例えば、「売り上げが１０％増え、粗利率も５％上昇する」など、実際に目にみえる効果を上げることにより、顧客にとってPalantirを「なくてはならないパートナー」にすることにあります。

【質問】

10月7日号でAI、半導体の話題にふれていただいていたと思います。先端半導体開発競争の中にラピダスが日本政府のバックアップを得ながら飛び込んでいこうとしています。

一方、同社は追う側であり、2nmでは後発、量産時期も競合に比べて遅いとの認識です。政府がこれだけの支援を行うには経済安全保障の観点があるのだと思うのですが、事業会社である以上、ビジネスとして成立させる必要があります。ラピダスに勝機はあるのか、あるとすればどんなところなのかお考えをお聞かせください。

《回答》

勝算はゼロではありませんが、トータルで数兆円の投資が必要なハイリスク・ハイリターンなビジネスです。数兆円の投資をしたところで、全く成果を出せない可能性も十分にあります。

難しいのは、これだけ難しいビジネスだと、他から資金を調達することが難しいため、日本政府が資金を提供し続けなければならない点です。兆円単位のお金を投入しながら、簡単には成果が出ない状況が続くと、必ず「これ以上国民の血税を注ぎ込む必要がるのか」という意見も出てくるし、逆に「ここまで莫大な税金を投入して来たのだから、今更やめられない」という意見も出てきて、純粋な投資案件として冷静な判断ができなくなってしまいます。

さらに悪いのは、ラピダスへの投資を決めた政治家や官僚は、その責任を取る立場にはなく、失敗しても痛くも痒くもないという状況にある点です。

シリコン・バレーのVCたちは、ラピダスのようなハイリスク・ハイリターンなビジネスへの投資を商売にしていますが、「ここまで莫大な投資をして来たから、今更やめられない」という状況には陥らないような設計になっているし（ステージごとに、進捗状況を見て、異なるVCが投資判断をします）、ファンドをマネージしている人たちも自分たちのお金をファンドに提供しているため、当事者意識が強いのです。

ラピダスの一番の懸念は、この座組みの悪さです。兆円単位の投資をするにも関わらず、投資判断をする政治家や官僚は、失敗しても痛くも痒くもない。こんな座組みで、冷静な投資判断ができるとは私には思えません。

【質問】

管理職に任命されたとき、だいぶ上の上司から「”独りよがり”と”信念を持つこと”は違うからね」というアドバイスを貰いました。この違いや線引き、ニュアンスについて、分かるような、分からないような、何とも言えない感覚で今もいます。中島さんの経験からみたとき、この言葉の解釈や信憑性を教えていただきたいです。

《回答》

現実歪曲空間を作り出すカリスマ経営者と詐欺師が紙一重であるのと同様に、信念を持つことと独りよがりも紙一重です。結局のところは、あなたが持つ信念をどのくらいの説得力を持って伝えられるかで勝負であり、説得力が欠如していれば、独りよがりと呼ばれてしまいます。

【質問】

現在、私は社内で生成AIの活用を推進する部門に所属しています。

生成AIを利用している社員は少数にとどまっており、今後さらに利用を促進し、業務の生産性向上を目指していますが、ただ、どのように生成AIの活用を進めていくべきか悩んでおります。特に悩みは生成AIツールの利用可否です。

現在は自社用に構築したChatGPTとMS Copilotを公認ツールとして利用を許可しています。しかし、それ以外のツールの使用は原則禁止としています。

当初はプロンプトに機密情報や個人情報を入力した場合の情報漏洩を懸念したためでしたが、最近になってGitHub CopilotやClaude Codeなど、他のAIツール利用を希望する声が上がってきています。

今後、新たに便利なツールが登場する可能性がある中で、それらを一律に禁止するのは非効率に感じています。今は、ある程度、利用規約などを確認し、情報漏洩のリスクが低いと判断した場合には、どんどん利用許可していくべきでしょうか？

中島さんがこのような立場であった場合、どのような方針で推進されるか、ご意見やアドバイスをいただけますと幸いです。

《回答》

業種や扱う情報ごとに異なる話なので、一概には言えませんが、まずは、データを

絶対に漏洩させていけない情報
漏洩させたくはないが、万が一AI経由で漏洩しても致命的ではない情報
漏洩させても問題ない情報
の三つに分類し、特に一番目のカテゴリーに属する情報（顧客のプライバシーに関わる情報、他社と結んだ機密保持契約でカバーされる情報、株価に影響を与える情報など）に関しては、決して外部のAPIに流さない体制を作ることからスタートすべきだと思います。会社としてこの線引きをはっきりとさせ、情報にアクセスできる人を制限する、アクセスできる人には重要性をしっかりと認識させ、情報漏洩を防ぎつつ働きやすい環境を会社として整えることが大切です。

そこさえしっかりとしておけば、二番目のカテゴリーに属する情報に関しては、各部署、もしくは、担当者レベルの判断に任せるのが良いと思います。

開発中のソフトウェアのソースコードなどをどのカテゴリーに入れるかは意見が分かれるところですが、私は二番目のカテゴリー扱いで良いと思います。もちろん、ソースコードが丸ごとそのまま漏洩するようなことは避けるべきですが、Claude CodeやCodexなどのコーディング・アシスタントを使うことにまで神経質になる必要はないと私は思います。

ちなみに、オープンソースな大規模言語モデルもかなり優秀になっているので、それを活用するのも一つの方法です。特に、OpenAIの「gpt-oss:20B」は、扱いやすい大きさにも関わらず優秀で、function callingがしっかりと動くので、社内の機密情報を扱う際には、安心して使えます。

試しに、開発中のMulmoChatをローカルで動く「gpt-oss:20B」と繋げ、上記の質問を投げて回答を作らせたところ、以下のようになりました（スペースを節約するためにPDF化してサーバーにおいておきました）。

生成AIツール活用の推進方針

【質問】

最近OpenAIが発表した「Apps SDK」に非常に興味を持っていますし少し触ってみましたがワンストップでAuthや決済ができるとなると面白いユーザー体験だなと思いました。

MCPとの連携で、AIをアプリの一部として直接組み込める点が革新的だと感じています。今後このアプリの開発がスケールしていくと考えられますか？

《回答》

どのくらいの規模になるかを予想することは簡単ではありませんが、全世界で数億人が使うChatGPT上の仕組みなので、数多くの開発者がアプリを作り、それなりのエコシステムが生まれる可能性は十分にあります。

しかし、これよりもさらに大きなポテンシャルを持つのは、（OpenAIが提供すると噂されている）「ChatGPTでログイン」した上でサードパーティのアプリやウェブサイト上から、ユーザーのアカウントでOpenAIのAPIを使う機能です。もしOpenAIがこれを提供すれば、爆発的な人気になることは確実です。

私自身、MulmoCastやMulmoChatを無料サービスとして提供したいものの、API課金の部分をこちらが負担しなければならない限り、無料での提供は難しく、月額課金制などの厄介なビジネスを立ち上げなければなりません。OpenAIが「ChatGPTでログイン」機能を提供してくれれば、その悩みがなくなります。

【質問】

私は消防士をしておりますが、ここ数年、電気を原因とする火災が増加しており、特にリチウムイオン電池に関連する火災が非常に多く発生しています。火災となったリチウムイオン電池を詳細に調査すると、その多くが安価で粗悪な製品による製造不良が原因であることが分かっています。

テレビなどのメディアでも注意喚起は行われていますが、これだけ世の中に普及している現状を踏まえると、今後さらに火災件数が増えるのではないかと危惧しています。

そこで、中島様にお伺いしたいことが2点ございます。

1点目は、中島様がお住まいの海外でも、リチウムイオン電池による火災は増加傾向にあるのでしょうか。もし増えている場合、行政としてどのような対策を講じているのか、教えていただけますと幸いです。
2点目は、リチウムイオン電池に代わる技術として全固体電池の開発が進められていると伺いました。
ChatGPTに尋ねたところ、日本やアメリカの企業がいくつか挙げられましたが、中島様が特に注目されている企業があれば、ぜひご教示いただけますとありがたく存じます。

《回答》

米国では、旅客機におけるリチウムイオン電池による火災が数多く発生しており、大きな問題になっています。貨物室に預ける荷物に入れることを禁止したり、機内では扱いに注意するなどの注意勧告はされていますが、飛行中に火災が起きると大ごとなので、関係者たちはとても神経質になっています。

確かに全固体電池の開発は進められていますが、まだ実用化の段階には来ていないのと、最初の応用は電気自動車になるだろうというのが私の理解です。「全固体電池へのシフト」を「大きな流れ」と捉えて、その市場で成功しそうな企業に長期投資するのは悪い戦略ではないと思います。ちなみに、私自身は、全固体電池を意識した投資はしていません。

【質問】

中島さんがAIを使用する際の裏技や気にかけているプロンプトなどあれば教えて頂きたく。例えば私はタスク登録してこの時間に天気を調べてなど使っております。

《回答》

私は、今や仕事のあらゆる場面でAIを使っていますが、特定のプロンプトを使うのではなく、対話しながらLLMの能力を引き出すようにしています。必要なプロンプトは要件ごとに異なるので、一回しか使わないプロンプトを時間をかけて作るよりは、対話型で指示を出したほうが、効率が良いのです。

ちなみに、Claude Codeなどを使ってAIにコードを書かせる際には、別のテクニックがありますが、それに関しては、上の方に詳しく解説したので、そちらをご覧ください。

【質問】

私は普段、臨床医として勤務しております。業務の多くは標準化可能なルーチンであり、適切に整備されたマニュアルがあれば、相当部分を効率化できるのではないかと考えています（もちろん、経験に基づくさじ加減は必要です）。

そこで、手元の教科書やマニュアル（1ファイル約1GB x 数十冊分）をOCR処理したPDFにし、必要なときに情報を取り出せる仕組みの構築を検討しています。ただ、ChatGPT・Gemini・Perplexityなどの日常的なAIサービスは、一度に直接アップロードできる容量やファイル数に制約があり、運用上の障壁となっています。

もし中島様でしたら、この課題をどのように解決されますでしょうか。RAGの構築を考えておりますが、恥ずかしながら私はPythonやITに不案内で、ChatGPTやClaudeに相談しても実装まで至っておりません。データベースの選定やシステム全体の設計方針について、アドバイスを頂戴できましたら幸いです。

(個人でも実現可能な程度に安価だと、とても助かります)

《回答》

私だったら、

OCR処理したPDFをセクションごとに分割
セクションごとに要約を生成
サマリーのEmbedding Vectorを作成
という前処理をあらかじめ行なっており、チャットをする際には、

質問文章のEmbedding Vectorを作る
そのVectorと距離が近いセクションを選び出す
それらのセクションをシステムプロンプトとしてLLMに提供
そのLLMに対して、質問文を投げる
という形で実装するだろうと思います。

【追記】ニーズがあるのであれば、MulmoChatにこの機能を追加するのも悪くないと考えています。汎用的なサービスとして作るのであれば、結構大変ですが、個人が使う程度であれば、それほど難しくないので、作って、オープンソースな形で公開するのも悪くないかも知れません。

【質問】

コンサルや外資系金融など、PowerPointやExcelなどを一日中使う職種で、かつ時間と闘っている人たちは、PCで作業する際にマウスを使わない人たちばかりのようです。マウスに手を伸ばす→クリックする→キーボードに戻るという反復が生産性の敵になるからというのが理由と聞きました。

中島さんはPCを使う時、マウスは利用されていますか？コンサル・外資系金融業界に限らず、仕事ができる人たちがどうしているのか気になっての質問です。

《回答》

私は、マウスではなく、MacBookのトラックパッドを使います。エンジニアの中には、ショートカットキーを多用することにより、トラックパッドを極力使わないようにしている人もいるようですが、私は、トラックパッドなしでは仕事ができません。マウスでもたぶん仕事はできると思いますが、今はトラックパッドに慣れています。

【質問】

ルミナーテクノロジーの話ですが、なぜこんなにも売られているのでしょうか？　2017年か2018年あたりにテスラが深刻な時期から脱却した～のような発言をされていたかと思いますが、ルミナーテクノロジーにとってその深刻なものがあるのでしょうか？　あるとしたら何ですか？まるでイーロンマスクがルミナーテクノロジーの株を永遠に空売りしているかのようなしつこさで株価が下がり続けています。それともルミナーテクノロジーは嫌われているんですか？

《回答》

個別株の株価を予想することはしませんが、LiDARマーケットには、

そもそも自動運転にLiDARが必要ないかも知れない
LiDARを普及させるには値段を大幅に下げる必要がある
複数の会社が競い合う市場である
という三つのリスクがあり、私自身はあまり魅力を感じていません。自動運転にLiDARが本当に必要かどうかの答えが出るのはこれからですが、幅広く導入されるためには、競争原理により値段を下げてコモディティ化をする必要があり、どちらにしても儲からないビジネスのように私には思えるのです。

【質問】

先日テレビ東京の番組で、孫正義氏がWayveの自動運転を体験する様子を視聴しました。カメラとAIでのアプローチで、Waymoのような高価格ではなく、15万円程度取付可能とのことでした。Wayveのアプローチ方と将来性、もし上場した場合、大株主とされるSBGの株価への影響等のご意見をいただけると幸いです。

《回答》

私も見ましたが、悪くないと思います。自動運転の難しさは、最後の1～2%の完成度を上げる部分であり、WaymoやTeslaですら苦労しています。完成度を上げるためには、大量のデータと、莫大な計算量が必要であり、資金力が乏しい中小ベンチャーにとっては、そこが一番難しいところです。孫さんがどのくらいの覚悟で、この市場に資金を投入してくるかによって、結果は大きく異なると思います。

ちなみに、SBGの株価についての予想はしませんが、ロボット事業やOpenAIなど、孫さんの目の付け所と度胸は天下一品だと思います。可能性がどのくらいあるかを予想することは不可能ですが、日本社会にとって、孫さんのようなリーダーは不可欠だと私は思います（ちなみに、私は現時点ではSBGの株は所有していません）。

【質問】

【前提】AIを「情報収集ツール」として利用するか、「思考の拡張ツール」として利用するか、主軸の違い（幼少期からのAI活用の差）が、将来の能力・キャリア差を拡大する傾向があると感じます。ここで、「思考の拡張」はAI支援による「仮説生成、反証探索、評価基準策定、意思決定、別領域への転移」を指すとします。

【質問】上記の前提の上で、下記2点をお伺いします（特にQ2に関心があります）。

国内の学習格差について、現状と将来的な影響の見立て、そして今後3年以内に取るべき具体的アクションをお聞かせください。
海外との学習格差が日本の国際競争力に与える影響について、主要因と長期的な影響、さらに今後3年以内に取るべき具体的アクションをお聞かせください。
《回答》

この話は、日本の「詰め込み教育」とも関連する話なので、とても重要ですが、このコーナーで回答できるような簡単な話ではありません。一つだけ確実に言えることは、日本型の「詰め込み教育」は、「豊富な知識をいつでも引き出せるAI」が人々のパートナーになった時代には、明らかに時代遅れだ、ということです。

今後は、「AIを活用してどこまで自分の能力を高めることができるか」が人材としての価値を決める時代になることは明らかで、そんな時代に相応しい人材を育成するには、極端な話、小学生の段階からAIに触れさせ、「パートナーとしてのAI」の存在を前提とした「AIネイティブ」な世代の育成に大きく舵を切るべきだと私は考えます。

【質問】

音声UIの発展を心から応援しています。我が家でも、料理中にアレクサへ買い物リストを追加したり、子どもたちが自分でタイマーをセットしてゲーム時間を管理したりと、音声操作の便利さを日々実感しています。

一方で、在宅ワークやゲームのボイスチャットなど、生活の中で“声を使う場面”が増えるほど、家の中に常に音がある状態に悩むこともあります。音声入力が主流になると、「静けさをどうデザインするか」という課題もますます重要になっていくように感じています。（私は自宅をリフォームするかヤマハの防音室を購入するか検討中です、、）

快適に音声UIを取り入れていくために、今後どのような工夫や技術が求められると考えますか？中島さんの視点でのお話、ぜひ伺ってみたいです。

《回答》

とても重要な話ですが。私も、先日、MulmoChatを知り合いにデモしたのですが、雑音の多いスタバでは出来ず、私の家に彼を招いてデモを行いました。

そう考えると、指向性マイク、それも、複数のマイクロフォンからの入力を合成することにより、任意のポイントからの音だけを拾うような技術がとても重要になると思います。

先日公開した、MulmoChat専用デバイスのプロトタイプには二つのカメラを設置しましたが、これは二つのカメラにより話者の口の位置を三次元空間で特定し、そこからの音をピンポイントに拾うことを意識した設計です。
